{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a70cfa28-551f-4a93-9139-3a9f066d93c2",
   "metadata": {},
   "source": [
    " Hate Speech Detector (Hindi and Tamil) \n",
    " To detect hate/offensive speech in Hindi and Tamil using `XLM-RoBERTa'\n",
    " ğŸ” Use Case\n",
    "- Moderate content on multilingual Indian social platforms (Facebook, ShareChat)\n",
    "- Prevent caste, religious, and political hate speech\n",
    "- Useful for detecting hate in code-mixed regional content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df922e5e-95b8-4bec-bf74-71869fb3d688",
   "metadata": {},
   "outputs": [],
   "source": [
    "!jupyter nbextension enable --py widgetsnbextension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc2f417-c5e6-4830-85cb-c7d55b01328a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --force-reinstall sympy==1.13.1\n",
    "print(\"Success\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d80acb91-bfc5-4c84-b2a3-ef2b421bfe45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SymPy version: 1.13.1\n",
      "mpmath version: 1.3.0\n",
      "Success - packages imported correctly!\n"
     ]
    }
   ],
   "source": [
    "import sympy\n",
    "import mpmath\n",
    "print(f\"SymPy version: {sympy.__version__}\")\n",
    "print(f\"mpmath version: {mpmath.__version__}\")\n",
    "print(\"Success - packages imported correctly!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6814d807-622f-4134-9812-e83ca9d29eff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in c:\\users\\mathu\\appdata\\roaming\\python\\python312\\site-packages (4.52.4)\n",
      "Requirement already satisfied: datasets in c:\\users\\mathu\\appdata\\roaming\\python\\python312\\site-packages (3.6.0)\n",
      "Requirement already satisfied: nltk in c:\\programdata\\anaconda3\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\programdata\\anaconda3\\lib\\site-packages (1.5.1)\n",
      "Requirement already satisfied: pandas in c:\\programdata\\anaconda3\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in c:\\users\\mathu\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (0.32.3)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\mathu\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\mathu\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from datasets) (16.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: xxhash in c:\\users\\mathu\\appdata\\roaming\\python\\python312\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\mathu\\appdata\\roaming\\python\\python312\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2024.6.1)\n",
      "Requirement already satisfied: click in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\programdata\\anaconda3\\lib\\site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.10.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.11.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (2024.8.30)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.11.0)\n",
      "Success\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers datasets nltk scikit-learn pandas\n",
    "print(\"Success\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e5f862-c723-44fd-a12f-7d294242129c",
   "metadata": {},
   "source": [
    "ğŸ“¥ Load and Combine Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4f026cf-3bc9-4899-951b-c75d4aba6087",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: numpy<2 in c:\\programdata\\anaconda3\\lib\\site-packages (1.26.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install \"numpy<2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6bc37b91-eb39-44b3-86fd-be19aaeb0425",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Both packages working!\n",
      "NumPy version: 1.26.4\n",
      "Pandas version: 2.2.2\n",
      "NumPy location: C:\\ProgramData\\anaconda3\\Lib\\site-packages\\numpy\\__init__.py\n",
      "Pandas location: C:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\__init__.py\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "print(\"âœ“ Both packages working!\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"NumPy location: {np.__file__}\")\n",
    "print(f\"Pandas location: {pd.__file__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fdc076bf-49aa-463a-b583-aa27fb5992af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy version: 1.26.4\n",
      "NumPy location: C:\\ProgramData\\anaconda3\\Lib\\site-packages\\numpy\\__init__.py\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"NumPy location: {np.__file__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "910a97ec-ba1e-46e1-ac43-4851464e0108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Unique ID', 'Post', 'Labels Set'], dtype='object')\n",
      "Index(['Unique ID', 'Post', 'Labels Set'], dtype='object')\n",
      "Read successfully\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "hindi_train = pd.read_csv(\"Hatespeech-Hindi_Train.csv\")[[\"Post\", \"Labels Set\"]]\n",
    "hindi_val = pd.read_csv(\"Hatespeech-Hindi_Valid.csv\")[[\"Post\", \"Labels Set\"]]\n",
    "print(pd.read_csv(\"Hatespeech-Hindi_Train.csv\").columns)\n",
    "print(pd.read_csv(\"Hatespeech-Hindi_Valid.csv\").columns)\n",
    "print(\"Read successfully\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0a6d183f-c34e-47de-93b8-876cfcc7360a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Renamed Successfully\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>lang</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>omg that bgm make me goosebumb...</td>\n",
       "      <td>0</td>\n",
       "      <td>ta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>neraya neraya neraya neraya neraya neraya.</td>\n",
       "      <td>0</td>\n",
       "      <td>ta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>thalaivar mersal look .semma massss thalaiva ....</td>\n",
       "      <td>0</td>\n",
       "      <td>ta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>paaaa... repeat mode.... adra adra adraaaaa......</td>\n",
       "      <td>0</td>\n",
       "      <td>ta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>epaa ena panaporam... sweet sapade poram... aw...</td>\n",
       "      <td>0</td>\n",
       "      <td>ta</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text label lang\n",
       "0                  omg that bgm make me goosebumb...     0   ta\n",
       "1         neraya neraya neraya neraya neraya neraya.     0   ta\n",
       "2  thalaivar mersal look .semma massss thalaiva ....     0   ta\n",
       "3  paaaa... repeat mode.... adra adra adraaaaa......     0   ta\n",
       "4  epaa ena panaporam... sweet sapade poram... aw...     0   ta"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Tamil files\n",
    "tamil_train = pd.read_csv(\"tamil_offensive_speech_train.csv\")[[\"comment\", \"label\"]]\n",
    "tamil_val = pd.read_csv(\"tamil_offensive_speech_val.csv\")[[\"comment\", \"label\"]]\n",
    "\n",
    "# Rename 'comment' column to 'text' to be consistent with Hindi dataset\n",
    "tamil_train = tamil_train.rename(columns={'comment': 'text'})\n",
    "tamil_val = tamil_val.rename(columns={'comment': 'text'})\n",
    "tamil_train[\"lang\"] = \"ta\"\n",
    "tamil_val[\"lang\"] = \"ta\"\n",
    "\n",
    "# Hindi files\n",
    "hindi_train = pd.read_csv(\"Hatespeech-Hindi_Train.csv\")[[\"Post\", \"Labels Set\"]]\n",
    "hindi_val = pd.read_csv(\"Hatespeech-Hindi_Valid.csv\")[[\"Post\", \"Labels Set\"]]\n",
    "\n",
    "# Rename columns to be consistent\n",
    "hindi_train = hindi_train.rename(columns={'Post': 'text', 'Labels Set': 'label'})\n",
    "hindi_val = hindi_val.rename(columns={'Post': 'text', 'Labels Set': 'label'})\n",
    "print(\"Renamed Successfully\")\n",
    "\n",
    "hindi_train[\"lang\"] = \"hi\"\n",
    "hindi_val[\"lang\"] = \"hi\"\n",
    "\n",
    "# Combine into df\n",
    "df = pd.concat([tamil_train, tamil_val, hindi_train, hindi_val], ignore_index=True)\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# View data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49356e19-e6d3-4c47-b346-10cd6e50ca65",
   "metadata": {},
   "source": [
    "ğŸ§¹ Clean and Preprocess Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "28491c3f-526b-41ff-8a32-020dada5bf61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['label', 'text'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load your dataset\n",
    "df = pd.read_csv(\"tamil_offensive_speech_train.csv\")\n",
    "\n",
    "# Rename 'comment' to 'text'\n",
    "df = df.rename(columns={'comment': 'text'})\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "85238ded-402a-4a9b-961b-f82e23b00507",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows with missing labels: 0\n",
      "Final sample:\n",
      "   label                                               text\n",
      "0      0                     omg that bgm make me goosebumb\n",
      "1      0          neraya neraya neraya neraya neraya neraya\n",
      "2      0  thalaivar mersal look semma massss thalaiva th...\n",
      "3      0  paaaa repeat mode adra adra adraaaaa vera leve...\n",
      "4      0  epaa ena panaporam sweet sapade poram awesome ...\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "def clean_text(text):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "# Assuming df is your dataframe and has columns 'text' and 'label'\n",
    "\n",
    "# Clean text column\n",
    "df['text'] = df['text'].apply(clean_text)\n",
    "\n",
    "def map_label(label):\n",
    "    if pd.isna(label):\n",
    "        return None\n",
    "    label = str(label).lower()\n",
    "\n",
    "    if label in ['0', 'non-hostile', 'normal']:\n",
    "        return 0\n",
    "    elif 'hate' in label:\n",
    "        return 2\n",
    "    elif any(word in label for word in ['offensive', 'defamation', 'fake']):\n",
    "        return 1\n",
    "    elif label in ['1']:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0  # default to normal if unclear\n",
    "\n",
    "df['label'] = df['label'].apply(map_label)\n",
    "\n",
    "print(f\"Rows with missing labels: {df['label'].isna().sum()}\")\n",
    "df.dropna(subset=['label'], inplace=True)\n",
    "df['label'] = df['label'].astype(int)\n",
    "\n",
    "# Drop rows with empty or missing text\n",
    "df = df[df['text'].str.strip() != '']\n",
    "\n",
    "print(\"Final sample:\")\n",
    "print(df.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee647546-543f-48be-a173-7de05e4f2962",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    df['text'], df['label'], test_size=0.2, stratify=df['label'], random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5ee5c108-7449-4cf0-96aa-d98ef916df57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "0    21224\n",
      "1     6649\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df['label'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "593548ac-8cfd-401c-90ab-cff8d780e4dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 22298\n",
      "Validation size: 5575\n",
      "Train label distribution:\n",
      " label\n",
      "0    16979\n",
      "1     5319\n",
      "Name: count, dtype: int64\n",
      "Validation label distribution:\n",
      " label\n",
      "0    4245\n",
      "1    1330\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Train size:\", len(train_texts))\n",
    "print(\"Validation size:\", len(val_texts))\n",
    "print(\"Train label distribution:\\n\", pd.Series(train_labels).value_counts())\n",
    "print(\"Validation label distribution:\\n\", pd.Series(val_labels).value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4ec89f-0542-4bf8-9acd-df4ae21bf94f",
   "metadata": {},
   "source": [
    "ğŸ¤– Tokenize using XLM-RoBERTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6fbc3bd4-7a21-4c66-8ee8-412f177f00f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torch in c:\\users\\mathu\\appdata\\roaming\\python\\python312\\site-packages (2.7.0)\n",
      "Requirement already satisfied: torchvision in c:\\users\\mathu\\appdata\\roaming\\python\\python312\\site-packages (0.22.0)\n",
      "Requirement already satisfied: torchaudio in c:\\users\\mathu\\appdata\\roaming\\python\\python312\\site-packages (2.7.0)\n",
      "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\mathu\\appdata\\roaming\\python\\python312\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (75.1.0)\n",
      "Requirement already satisfied: numpy in c:\\programdata\\anaconda3\\lib\\site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from torchvision) (10.4.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\mathu\\appdata\\roaming\\python\\python312\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5fe0d63f-aa61-42c9-81d5-a605bd13bc78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.52.4\n",
      "2.7.0+cpu\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "print(transformers.__version__)\n",
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c559059e-6960-47d9-b94d-f597fbc6c78a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\python.exe\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "72042b28-e54d-403a-b9b5-3ba96ec157f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torch in c:\\users\\mathu\\appdata\\roaming\\python\\python312\\site-packages (2.7.0)\n",
      "Collecting torchvision\n",
      "  Downloading torchvision-0.22.0-cp312-cp312-win_amd64.whl.metadata (6.3 kB)\n",
      "Collecting torchaudio\n",
      "  Downloading torchaudio-2.7.0-cp312-cp312-win_amd64.whl.metadata (6.7 kB)\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.52.4-py3-none-any.whl.metadata (38 kB)\n",
      "Requirement already satisfied: accelerate in c:\\users\\mathu\\appdata\\roaming\\python\\python312\\site-packages (1.7.0)\n",
      "Requirement already satisfied: datasets in c:\\users\\mathu\\appdata\\roaming\\python\\python312\\site-packages (3.6.0)\n",
      "Requirement already satisfied: pandas in c:\\programdata\\anaconda3\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: scikit-learn in c:\\programdata\\anaconda3\\lib\\site-packages (1.5.1)\n",
      "Requirement already satisfied: numpy in c:\\programdata\\anaconda3\\lib\\site-packages (1.26.4)\n",
      "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\mathu\\appdata\\roaming\\python\\python312\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (75.1.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from torchvision) (10.4.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in c:\\users\\mathu\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (0.32.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\mathu\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\mathu\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: psutil in c:\\programdata\\anaconda3\\lib\\site-packages (from accelerate) (5.9.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from datasets) (16.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: xxhash in c:\\users\\mathu\\appdata\\roaming\\python\\python312\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\mathu\\appdata\\roaming\\python\\python312\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\programdata\\anaconda3\\lib\\site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.10.5)\n",
      "Requirement already satisfied: six>=1.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (2024.8.30)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\mathu\\appdata\\roaming\\python\\python312\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.11.0)\n",
      "Downloading torchvision-0.22.0-cp312-cp312-win_amd64.whl (1.7 MB)\n",
      "   ---------------------------------------- 0.0/1.7 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 0.3/1.7 MB ? eta -:--:--\n",
      "   ------------------------ --------------- 1.0/1.7 MB 3.1 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 1.6/1.7 MB 2.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.7/1.7 MB 2.7 MB/s eta 0:00:00\n",
      "Downloading torchaudio-2.7.0-cp312-cp312-win_amd64.whl (2.5 MB)\n",
      "   ---------------------------------------- 0.0/2.5 MB ? eta -:--:--\n",
      "   -------- ------------------------------- 0.5/2.5 MB 2.4 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 1.0/2.5 MB 2.2 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 1.8/2.5 MB 2.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.5/2.5 MB 3.0 MB/s eta 0:00:00\n",
      "Downloading transformers-4.52.4-py3-none-any.whl (10.5 MB)\n",
      "   ---------------------------------------- 0.0/10.5 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 0.8/10.5 MB 6.7 MB/s eta 0:00:02\n",
      "   ------ --------------------------------- 1.6/10.5 MB 3.8 MB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 2.6/10.5 MB 4.1 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 3.9/10.5 MB 4.7 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 4.7/10.5 MB 4.6 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 6.0/10.5 MB 4.9 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 7.6/10.5 MB 5.2 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 8.7/10.5 MB 5.2 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 9.7/10.5 MB 5.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  10.2/10.5 MB 4.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 10.5/10.5 MB 4.8 MB/s eta 0:00:00\n",
      "Installing collected packages: torchvision, torchaudio, transformers\n",
      "Successfully installed torchaudio-2.7.0 torchvision-0.22.0 transformers-4.52.4\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision torchaudio transformers accelerate datasets pandas scikit-learn numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f11cb67d-fcc3-4f89-9d96-ab74b31bb8a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.12.7 | packaged by Anaconda, Inc. | (main, Oct  4 2024, 13:17:27) [MSC v.1929 64 bit (AMD64)]\n",
      "âœ… transformers - OK\n",
      "âœ… accelerate - OK\n",
      "âœ… datasets - OK\n",
      "âœ… torch - OK\n",
      "âœ… pandas - OK\n",
      "âœ… sklearn - OK\n",
      "âœ… numpy - OK\n"
     ]
    }
   ],
   "source": [
    "# Test if everything is installed correctly\n",
    "import sys\n",
    "print(\"Python version:\", sys.version)\n",
    "\n",
    "packages_to_test = [\n",
    "    'transformers', 'accelerate', 'datasets', \n",
    "    'torch', 'pandas', 'sklearn', 'numpy'\n",
    "]\n",
    "\n",
    "for package in packages_to_test:\n",
    "    try:\n",
    "        __import__(package)\n",
    "        print(f\"âœ… {package} - OK\")\n",
    "    except ImportError as e:\n",
    "        print(f\"âŒ {package} - Missing: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9fc1a3a6-5f1c-436c-b258-6f0e4d5395e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformers version: 4.52.4\n",
      "Accelerate version: 1.7.0\n",
      "PyTorch version: 2.6.0+cpu\n",
      "CUDA available: False\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import accelerate\n",
    "import torch\n",
    "\n",
    "print(f\"Transformers version: {transformers.__version__}\")\n",
    "print(f\"Accelerate version: {accelerate.__version__}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "452b8384-5f04-4b64-9f7d-1d1a7efa4cb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting hf_xet\n",
      "  Downloading hf_xet-1.1.2-cp37-abi3-win_amd64.whl.metadata (883 bytes)\n",
      "Downloading hf_xet-1.1.2-cp37-abi3-win_amd64.whl (2.7 MB)\n",
      "   ---------------------------------------- 0.0/2.7 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 0.3/2.7 MB ? eta -:--:--\n",
      "   ------------------- -------------------- 1.3/2.7 MB 3.7 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 2.1/2.7 MB 4.3 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 2.6/2.7 MB 3.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.7/2.7 MB 3.4 MB/s eta 0:00:00\n",
      "Installing collected packages: hf_xet\n",
      "Successfully installed hf_xet-1.1.2\n",
      "âœ… All imports successful - no Trainer issues!\n",
      "\n",
      "ğŸ” Checking hardware...\n",
      "CUDA available: False\n",
      "Device count: 0\n",
      "âš ï¸  Using CPU - training will be slower\n",
      "ğŸŒŸ Multilingual Hate Speech Detection Training\n",
      "ğŸ”§ Optimized for CPU with sample data for fast testing\n",
      "================================================================================\n",
      "ğŸš€ Starting Multilingual Hate Speech Detection Training (Tamil + Hindi)\n",
      "================================================================================\n",
      "ğŸ“‚ Loading Tamil datasets...\n",
      "âœ… Tamil train: 27875 samples\n",
      "âœ… Tamil validation: 6969 samples\n",
      "ğŸ“Š Tamil label distribution:\n",
      "label\n",
      "0    21226\n",
      "1     6649\n",
      "Name: count, dtype: int64\n",
      "\n",
      "ğŸ“‚ Loading Hindi datasets...\n",
      "âœ… Hindi train: 5728 samples\n",
      "âœ… Hindi validation: 811 samples\n",
      "ğŸ“Š Original Hindi label distribution:\n",
      "label\n",
      "non-hostile                       3050\n",
      "fake                              1009\n",
      "hate                               478\n",
      "offensive                          405\n",
      "defamation                         305\n",
      "hate,offensive                     163\n",
      "defamation,offensive                81\n",
      "defamation,hate                     74\n",
      "defamation,fake                     34\n",
      "defamation,hate,offensive           28\n",
      "fake,offensive                      28\n",
      "fake,hate                           27\n",
      "defamation,fake,offensive           24\n",
      "defamation,fake,hate                 9\n",
      "defamation,fake,hate,offensive       9\n",
      "fake,hate,offensive                  4\n",
      "Name: count, dtype: int64\n",
      "ğŸ“Š Processed Hindi label distribution:\n",
      "label\n",
      "0    3050\n",
      "1    2678\n",
      "Name: count, dtype: int64\n",
      "\n",
      "ğŸ”„ Combining datasets...\n",
      "âœ… Combined dataset: 41378 samples\n",
      "ğŸ“Š Final label distribution:\n",
      "label\n",
      "0    29993\n",
      "1    11385\n",
      "Name: count, dtype: int64\n",
      "ğŸ“Š Language distribution:\n",
      "lang\n",
      "ta    34839\n",
      "hi     6539\n",
      "Name: count, dtype: int64\n",
      "\n",
      "ğŸ” Sample data:\n",
      "                                                text  label lang\n",
      "0                  omg that bgm make me goosebumb...      0   ta\n",
      "1         neraya neraya neraya neraya neraya neraya.      0   ta\n",
      "2  thalaivar mersal look .semma massss thalaiva ....      0   ta\n",
      "3  paaaa... repeat mode.... adra adra adraaaaa......      0   ta\n",
      "4  epaa ena panaporam... sweet sapade poram... aw...      0   ta\n",
      "\n",
      "ğŸ§ª Using sample for faster testing...\n",
      "Sample size: 5000 samples\n",
      "Sample label distribution:\n",
      "label\n",
      "0    3596\n",
      "1    1404\n",
      "Name: count, dtype: int64\n",
      "Sample language distribution:\n",
      "lang\n",
      "ta    4194\n",
      "hi     806\n",
      "Name: count, dtype: int64\n",
      "\n",
      "ğŸ”„ Splitting combined data...\n",
      "ğŸ“š Training samples: 4000\n",
      "ğŸ“ Validation samples: 1000\n",
      "\n",
      "ğŸ“Š Training Configuration:\n",
      "   ğŸ¤– Model: distilbert-base-multilingual-cased\n",
      "   ğŸ“¦ Batch size: 8\n",
      "   ğŸ”„ Epochs: 2\n",
      "   ğŸ“ Max length: 64\n",
      "   ğŸ“ˆ Learning rate: 2e-05\n",
      "\n",
      "ğŸ¤– Loading multilingual model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Model loaded with 2 labels\n",
      "\n",
      "ğŸ“¦ Creating datasets...\n",
      "âœ… Data loaders created\n",
      "   ğŸ“š Training batches: 500\n",
      "   ğŸ“ Validation batches: 125\n",
      "\n",
      "ğŸ’» Using device: cpu\n",
      "â±ï¸  Estimated training time: ~50 minutes\n",
      "\n",
      "ğŸ¯ Starting training...\n",
      "\n",
      "ğŸ“š Epoch 1/2\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [24:27<00:00,  2.93s/it, loss=0.9763]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Training Loss: 0.4894 | Accuracy: 0.7705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [01:01<00:00,  2.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ˆ Validation Loss: 0.4220 | Accuracy: 0.8080\n",
      "ğŸ† New best validation accuracy: 0.8080\n",
      "\n",
      "ğŸ“š Epoch 2/2\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [24:27<00:00,  2.94s/it, loss=0.1580]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Training Loss: 0.3708 | Accuracy: 0.8277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [01:02<00:00,  2.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ˆ Validation Loss: 0.4228 | Accuracy: 0.8190\n",
      "ğŸ† New best validation accuracy: 0.8190\n",
      "\n",
      "ğŸ“‹ Final Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Non-Hate       0.84      0.92      0.88       719\n",
      "        Hate       0.73      0.57      0.64       281\n",
      "\n",
      "    accuracy                           0.82      1000\n",
      "   macro avg       0.79      0.74      0.76      1000\n",
      "weighted avg       0.81      0.82      0.81      1000\n",
      "\n",
      "\n",
      "ğŸ’¾ Saving model to ./trained_multilingual_hate_speech_model...\n",
      "\n",
      "ğŸ‰ Training completed successfully!\n",
      "ğŸ“ Model saved to: ./trained_multilingual_hate_speech_model\n",
      "ğŸš€ You can now use this model for multilingual hate speech detection!\n",
      "\n",
      "ğŸ§ª Testing multilingual predictions:\n",
      "------------------------------------------------------------\n",
      "1. Text: 'à®‡à®¤à¯ à®’à®°à¯ à®¨à®²à¯à®² à®šà¯†à®¯à¯à®¤à®¿'\n",
      "   Prediction: Non-Hate (Confidence: 0.9728)\n",
      "\n",
      "2. Text: 'à®…à®°à¯à®®à¯ˆà®¯à®¾à®© à®µà¯‡à®²à¯ˆ'\n",
      "   Prediction: Non-Hate (Confidence: 0.9596)\n",
      "\n",
      "3. Text: 'à®µà®¾à®´à¯à®¤à¯à®¤à¯à®•à¯à®•à®³à¯'\n",
      "   Prediction: Non-Hate (Confidence: 0.9866)\n",
      "\n",
      "4. Text: 'à¤¯à¤¹ à¤¬à¤¹à¥à¤¤ à¤…à¤šà¥à¤›à¤¾ à¤¹à¥ˆ'\n",
      "   Prediction: Hate (Confidence: 0.9501)\n",
      "\n",
      "5. Text: 'à¤¬à¤¹à¥à¤¤ à¤¬à¤¢à¤¼à¤¿à¤¯à¤¾ à¤•à¤¾à¤®'\n",
      "   Prediction: Hate (Confidence: 0.8974)\n",
      "\n",
      "6. Text: 'à¤§à¤¨à¥à¤¯à¤µà¤¾à¤¦'\n",
      "   Prediction: Non-Hate (Confidence: 0.6652)\n",
      "\n",
      "7. Text: 'Great job! à¤¬à¤¹à¥à¤¤ à¤…à¤šà¥à¤›à¤¾'\n",
      "   Prediction: Hate (Confidence: 0.6365)\n",
      "\n",
      "8. Text: 'à®µà®¾à®´à¯à®¤à¯à®¤à¯à®•à¯à®•à®³à¯ friend!'\n",
      "   Prediction: Non-Hate (Confidence: 0.9833)\n",
      "\n",
      "\n",
      "================================================================================\n",
      "ğŸ¯ Training Summary:\n",
      "âœ… Model: DistilBERT Multilingual (optimized for speed)\n",
      "âœ… Languages: Tamil + Hindi\n",
      "âœ… Task: Binary hate speech classification\n",
      "âœ… Optimization: Sample data + reduced parameters for CPU\n",
      "ğŸ’¡ To use full dataset: Set USE_SAMPLE = False in load_and_prepare_combined_data()\n",
      "ğŸ’¡ For production: Use GPU and increase epochs to 5+\n"
     ]
    }
   ],
   "source": [
    "!pip install hf_xet\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset as TorchDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "from datasets import Dataset\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set environment\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "print(\"âœ… All imports successful - no Trainer issues!\")\n",
    "\n",
    "# Hardware check\n",
    "print(\"\\nğŸ” Checking hardware...\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"Device count: {torch.cuda.device_count()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name()}\")\n",
    "else:\n",
    "    print(\"âš ï¸  Using CPU - training will be slower\")\n",
    "\n",
    "class CustomDataset(TorchDataset):\n",
    "    \"\"\"Custom PyTorch Dataset for text classification\"\"\"\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=64):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "def preprocess_hindi_labels(label_str):\n",
    "    \"\"\"Convert Hindi multi-label format to binary classification\"\"\"\n",
    "    if pd.isna(label_str) or label_str == 'non-hostile':\n",
    "        return 0  # Non-hate\n",
    "    else:\n",
    "        return 1  # Hate (any form of hate/offensive/defamation)\n",
    "\n",
    "def load_and_prepare_combined_data():\n",
    "    \"\"\"Load and prepare the combined Tamil-Hindi datasets\"\"\"\n",
    "    try:\n",
    "        print(\"ğŸ“‚ Loading Tamil datasets...\")\n",
    "        # Tamil files\n",
    "        tamil_train = pd.read_csv(\"tamil_offensive_speech_train.csv\")[[\"comment\", \"label\"]]\n",
    "        tamil_val = pd.read_csv(\"tamil_offensive_speech_val.csv\")[[\"comment\", \"label\"]]\n",
    "        \n",
    "        # Rename 'comment' column to 'text' to be consistent with Hindi dataset\n",
    "        tamil_train = tamil_train.rename(columns={'comment': 'text'})\n",
    "        tamil_val = tamil_val.rename(columns={'comment': 'text'})\n",
    "        tamil_train[\"lang\"] = \"ta\"\n",
    "        tamil_val[\"lang\"] = \"ta\"\n",
    "        \n",
    "        print(f\"âœ… Tamil train: {len(tamil_train)} samples\")\n",
    "        print(f\"âœ… Tamil validation: {len(tamil_val)} samples\")\n",
    "        print(f\"ğŸ“Š Tamil label distribution:\")\n",
    "        print(tamil_train['label'].value_counts())\n",
    "        \n",
    "        print(\"\\nğŸ“‚ Loading Hindi datasets...\")\n",
    "        # Hindi files\n",
    "        hindi_train = pd.read_csv(\"Hatespeech-Hindi_Train.csv\")[[\"Post\", \"Labels Set\"]]\n",
    "        hindi_val = pd.read_csv(\"Hatespeech-Hindi_Valid.csv\")[[\"Post\", \"Labels Set\"]]\n",
    "        \n",
    "        # Rename columns to be consistent\n",
    "        hindi_train = hindi_train.rename(columns={'Post': 'text', 'Labels Set': 'label'})\n",
    "        hindi_val = hindi_val.rename(columns={'Post': 'text', 'Labels Set': 'label'})\n",
    "        \n",
    "        print(f\"âœ… Hindi train: {len(hindi_train)} samples\")\n",
    "        print(f\"âœ… Hindi validation: {len(hindi_val)} samples\")\n",
    "        print(f\"ğŸ“Š Original Hindi label distribution:\")\n",
    "        print(hindi_train['label'].value_counts())\n",
    "        \n",
    "        # Process Hindi labels to binary format\n",
    "        hindi_train['label'] = hindi_train['label'].apply(preprocess_hindi_labels)\n",
    "        hindi_val['label'] = hindi_val['label'].apply(preprocess_hindi_labels)\n",
    "        \n",
    "        print(f\"ğŸ“Š Processed Hindi label distribution:\")\n",
    "        print(hindi_train['label'].value_counts())\n",
    "        \n",
    "        hindi_train[\"lang\"] = \"hi\"\n",
    "        hindi_val[\"lang\"] = \"hi\"\n",
    "        \n",
    "        print(\"\\nğŸ”„ Combining datasets...\")\n",
    "        # Combine into df\n",
    "        df = pd.concat([tamil_train, tamil_val, hindi_train, hindi_val], ignore_index=True)\n",
    "        df.dropna(inplace=True)\n",
    "        \n",
    "        print(f\"âœ… Combined dataset: {len(df)} samples\")\n",
    "        print(f\"ğŸ“Š Final label distribution:\")\n",
    "        print(df['label'].value_counts())\n",
    "        print(f\"ğŸ“Š Language distribution:\")\n",
    "        print(df['lang'].value_counts())\n",
    "        \n",
    "        # Display sample data\n",
    "        print(f\"\\nğŸ” Sample data:\")\n",
    "        print(df.head())\n",
    "        \n",
    "        # OPTIMIZATION: Use sample for faster testing\n",
    "        USE_SAMPLE = True  # Set to False for full training\n",
    "        \n",
    "        if USE_SAMPLE:\n",
    "            print(f\"\\nğŸ§ª Using sample for faster testing...\")\n",
    "            df_sample = df.sample(n=5000, random_state=42)\n",
    "            print(f\"Sample size: {len(df_sample)} samples\")\n",
    "            print(f\"Sample label distribution:\")\n",
    "            print(df_sample['label'].value_counts())\n",
    "            print(f\"Sample language distribution:\")\n",
    "            print(df_sample['lang'].value_counts())\n",
    "            return df_sample\n",
    "        else:\n",
    "            print(f\"\\nğŸ”¥ Using full dataset for training...\")\n",
    "            return df\n",
    "        \n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"âŒ File not found: {e}\")\n",
    "        print(\"ğŸ“ Make sure all CSV files are in the same directory\")\n",
    "        return None\n",
    "\n",
    "def train_model(model, train_dataloader, val_dataloader, device, epochs=2, lr=2e-5):\n",
    "    \"\"\"Manual training loop - replaces Trainer\"\"\"\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    total_steps = len(train_dataloader) * epochs\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=0,\n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "    \n",
    "    model.to(device)\n",
    "    best_val_accuracy = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        print(f'\\nğŸ“š Epoch {epoch + 1}/{epochs}')\n",
    "        print('-' * 50)\n",
    "        \n",
    "        # Training phase\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        train_predictions = []\n",
    "        train_true = []\n",
    "        \n",
    "        train_pbar = tqdm(train_dataloader, desc=\"Training\")\n",
    "        for batch in train_pbar:\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            outputs = model(input_ids=input_ids, \n",
    "                          attention_mask=attention_mask, \n",
    "                          labels=labels)\n",
    "            \n",
    "            loss = outputs.loss\n",
    "            total_train_loss += loss.item()\n",
    "            \n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "            train_predictions.extend(predictions.cpu().numpy())\n",
    "            train_true.extend(labels.cpu().numpy())\n",
    "            \n",
    "            train_pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "        \n",
    "        avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "        train_accuracy = accuracy_score(train_true, train_predictions)\n",
    "        \n",
    "        print(f'ğŸ“Š Training Loss: {avg_train_loss:.4f} | Accuracy: {train_accuracy:.4f}')\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        total_eval_loss = 0\n",
    "        eval_predictions = []\n",
    "        eval_true = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            val_pbar = tqdm(val_dataloader, desc=\"Validation\")\n",
    "            for batch in val_pbar:\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "                \n",
    "                outputs = model(input_ids=input_ids, \n",
    "                              attention_mask=attention_mask, \n",
    "                              labels=labels)\n",
    "                \n",
    "                loss = outputs.loss\n",
    "                total_eval_loss += loss.item()\n",
    "                \n",
    "                predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "                eval_predictions.extend(predictions.cpu().numpy())\n",
    "                eval_true.extend(labels.cpu().numpy())\n",
    "        \n",
    "        avg_val_loss = total_eval_loss / len(val_dataloader)\n",
    "        val_accuracy = accuracy_score(eval_true, eval_predictions)\n",
    "        \n",
    "        print(f'ğŸ“ˆ Validation Loss: {avg_val_loss:.4f} | Accuracy: {val_accuracy:.4f}')\n",
    "        \n",
    "        # Save best model\n",
    "        if val_accuracy > best_val_accuracy:\n",
    "            best_val_accuracy = val_accuracy\n",
    "            print(f'ğŸ† New best validation accuracy: {best_val_accuracy:.4f}')\n",
    "        \n",
    "        # Print classification report for last epoch\n",
    "        if epoch == epochs - 1:\n",
    "            print(\"\\nğŸ“‹ Final Classification Report:\")\n",
    "            print(classification_report(eval_true, eval_predictions, target_names=['Non-Hate', 'Hate']))\n",
    "    \n",
    "    return model\n",
    "\n",
    "def run_multilingual_hate_speech_training():\n",
    "    \"\"\"Main function to run multilingual hate speech detection training\"\"\"\n",
    "    \n",
    "    print(\"ğŸš€ Starting Multilingual Hate Speech Detection Training (Tamil + Hindi)\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # 1. Load and prepare combined data\n",
    "    df = load_and_prepare_combined_data()\n",
    "    \n",
    "    if df is None:\n",
    "        print(\"âŒ Failed to load data. Exiting...\")\n",
    "        return\n",
    "    \n",
    "    # 2. Split the data (stratified by both label and language if possible)\n",
    "    print(f\"\\nğŸ”„ Splitting combined data...\")\n",
    "    train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "        df['text'].tolist(),\n",
    "        df['label'].tolist(),\n",
    "        test_size=0.2,\n",
    "        random_state=42,\n",
    "        stratify=df['label']  # Stratify by label\n",
    "    )\n",
    "    \n",
    "    print(f\"ğŸ“š Training samples: {len(train_texts)}\")\n",
    "    print(f\"ğŸ“ Validation samples: {len(val_texts)}\")\n",
    "    \n",
    "    # 3. Optimized model and parameters\n",
    "    model_name = \"distilbert-base-multilingual-cased\"  # Fast multilingual model\n",
    "    batch_size = 8      # Reduced for CPU efficiency\n",
    "    epochs = 2          # Reduced for testing\n",
    "    max_length = 64     # Reduced for speed\n",
    "    learning_rate = 2e-5\n",
    "    \n",
    "    print(f\"\\nğŸ“Š Training Configuration:\")\n",
    "    print(f\"   ğŸ¤– Model: {model_name}\")\n",
    "    print(f\"   ğŸ“¦ Batch size: {batch_size}\")\n",
    "    print(f\"   ğŸ”„ Epochs: {epochs}\")\n",
    "    print(f\"   ğŸ“ Max length: {max_length}\")\n",
    "    print(f\"   ğŸ“ˆ Learning rate: {learning_rate}\")\n",
    "    \n",
    "    # 4. Load model and tokenizer\n",
    "    print(f\"\\nğŸ¤– Loading multilingual model...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "    num_labels = len(set(df['label']))\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_name,\n",
    "        num_labels=num_labels,\n",
    "        ignore_mismatched_sizes=True\n",
    "    )\n",
    "    \n",
    "    print(f\"âœ… Model loaded with {num_labels} labels\")\n",
    "    \n",
    "    # 5. Create datasets and data loaders\n",
    "    print(f\"\\nğŸ“¦ Creating datasets...\")\n",
    "    train_dataset = CustomDataset(train_texts, train_labels, tokenizer, max_length=max_length)\n",
    "    val_dataset = CustomDataset(val_texts, val_labels, tokenizer, max_length=max_length)\n",
    "    \n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    print(f\"âœ… Data loaders created\")\n",
    "    print(f\"   ğŸ“š Training batches: {len(train_dataloader)}\")\n",
    "    print(f\"   ğŸ“ Validation batches: {len(val_dataloader)}\")\n",
    "    \n",
    "    # 6. Set device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"\\nğŸ’» Using device: {device}\")\n",
    "    \n",
    "    # Estimate training time\n",
    "    if device.type == 'cpu':\n",
    "        estimated_time = len(train_dataloader) * epochs * 3  # ~3 seconds per batch on CPU\n",
    "        print(f\"â±ï¸  Estimated training time: ~{estimated_time//60} minutes\")\n",
    "    \n",
    "    # 7. Train the model\n",
    "    print(f\"\\nğŸ¯ Starting training...\")\n",
    "    trained_model = train_model(\n",
    "        model, \n",
    "        train_dataloader, \n",
    "        val_dataloader, \n",
    "        device,\n",
    "        epochs=epochs,\n",
    "        lr=learning_rate\n",
    "    )\n",
    "    \n",
    "    # 8. Save the model\n",
    "    output_dir = \"./trained_multilingual_hate_speech_model\"\n",
    "    print(f\"\\nğŸ’¾ Saving model to {output_dir}...\")\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    trained_model.save_pretrained(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "    \n",
    "    print(f\"\\nğŸ‰ Training completed successfully!\")\n",
    "    print(f\"ğŸ“ Model saved to: {output_dir}\")\n",
    "    print(f\"ğŸš€ You can now use this model for multilingual hate speech detection!\")\n",
    "    \n",
    "    return trained_model, tokenizer\n",
    "\n",
    "def test_multilingual_predictions(model, tokenizer, device):\n",
    "    \"\"\"Test the trained model on sample texts in both languages\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    test_texts = [\n",
    "        # Tamil examples (safe)\n",
    "        \"à®‡à®¤à¯ à®’à®°à¯ à®¨à®²à¯à®² à®šà¯†à®¯à¯à®¤à®¿\",  # This is good news\n",
    "        \"à®…à®°à¯à®®à¯ˆà®¯à®¾à®© à®µà¯‡à®²à¯ˆ\",      # Excellent work\n",
    "        \"à®µà®¾à®´à¯à®¤à¯à®¤à¯à®•à¯à®•à®³à¯\",        # Congratulations\n",
    "        \n",
    "        # Hindi examples (safe)\n",
    "        \"à¤¯à¤¹ à¤¬à¤¹à¥à¤¤ à¤…à¤šà¥à¤›à¤¾ à¤¹à¥ˆ\",      # This is very good\n",
    "        \"à¤¬à¤¹à¥à¤¤ à¤¬à¤¢à¤¼à¤¿à¤¯à¤¾ à¤•à¤¾à¤®\",       # Very good work\n",
    "        \"à¤§à¤¨à¥à¤¯à¤µà¤¾à¤¦\",              # Thank you\n",
    "        \n",
    "        # Mixed content for testing\n",
    "        \"Great job! à¤¬à¤¹à¥à¤¤ à¤…à¤šà¥à¤›à¤¾\",  # Mixed language\n",
    "        \"à®µà®¾à®´à¯à®¤à¯à®¤à¯à®•à¯à®•à®³à¯ friend!\"   # Mixed language\n",
    "    ]\n",
    "    \n",
    "    print(\"\\nğŸ§ª Testing multilingual predictions:\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for i, text in enumerate(test_texts, 1):\n",
    "        encoding = tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=64,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            input_ids = encoding['input_ids'].to(device)\n",
    "            attention_mask = encoding['attention_mask'].to(device)\n",
    "            \n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            prediction = torch.argmax(outputs.logits, dim=-1)\n",
    "            confidence = torch.softmax(outputs.logits, dim=-1)\n",
    "            \n",
    "        pred_label = \"Hate\" if prediction.item() == 1 else \"Non-Hate\"\n",
    "        conf_score = confidence.max().item()\n",
    "        \n",
    "        print(f\"{i}. Text: '{text}'\")\n",
    "        print(f\"   Prediction: {pred_label} (Confidence: {conf_score:.4f})\")\n",
    "        print()\n",
    "\n",
    "# Run the training\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"ğŸŒŸ Multilingual Hate Speech Detection Training\")\n",
    "    print(\"ğŸ”§ Optimized for CPU with sample data for fast testing\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    trained_model, tokenizer = run_multilingual_hate_speech_training()\n",
    "    \n",
    "    # Test the model if training was successful\n",
    "    if trained_model is not None:\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        test_multilingual_predictions(trained_model, tokenizer, device)\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"ğŸ¯ Training Summary:\")\n",
    "        print(\"âœ… Model: DistilBERT Multilingual (optimized for speed)\")\n",
    "        print(\"âœ… Languages: Tamil + Hindi\")\n",
    "        print(\"âœ… Task: Binary hate speech classification\")\n",
    "        print(\"âœ… Optimization: Sample data + reduced parameters for CPU\")\n",
    "        print(\"ğŸ’¡ To use full dataset: Set USE_SAMPLE = False in load_and_prepare_combined_data()\")\n",
    "        print(\"ğŸ’¡ For production: Use GPU and increase epochs to 5+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c973a03-8a98-4acc-94fb-e9f11678482f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Dataset loaded successfully!\n",
      "Raw dataset shape: (27875, 2)\n",
      "Columns: ['label', 'comment']\n",
      "Dataset shape: (27875, 2)\n",
      "Label distribution:\n",
      "label\n",
      "0    21226\n",
      "1     6649\n",
      "Name: count, dtype: int64\n",
      "âœ… Data preprocessing completed!\n"
     ]
    }
   ],
   "source": [
    "# âœ… Step 1: Load your dataset (choose one)\n",
    "# For Hindi dataset\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"Hatespeech-Hindi_Train.csv\")\n",
    "# For Tamil dataset\n",
    "df = pd.read_csv(\"tamil_offensive_speech_train.csv\")\n",
    "\n",
    "print(\"âœ… Dataset loaded successfully!\")\n",
    "print(f\"Raw dataset shape: {df.shape}\")\n",
    "print(f\"Columns: {list(df.columns)}\")\n",
    "# âœ… Step 2: Rename columns to standardize\n",
    "df = df.rename(columns={\n",
    "    'comment': 'text',    # Replace 'comment' with your actual column name\n",
    "    'category': 'label'   # Replace 'category' with your actual label column\n",
    "})\n",
    "\n",
    "# âœ… Step 3: Clean and prepare data\n",
    "df = df[['text', 'label']]  # Keep only relevant columns\n",
    "df['text'] = df['text'].astype(str)\n",
    "df['label'] = df['label'].astype(int)\n",
    "\n",
    "# Remove any rows with missing values\n",
    "df = df.dropna()\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Label distribution:\\n{df['label'].value_counts()}\")\n",
    "print(\"âœ… Data preprocessing completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f469cee8-20bc-4e86-bdba-af968a441d84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 22300\n",
      "Validation set size: 5575\n",
      "âœ… Train-validation split completed!\n"
     ]
    }
   ],
   "source": [
    "# âœ… Step 4: Split into train/validation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset\n",
    "train_df, val_df = train_test_split(\n",
    "    df, \n",
    "    test_size=0.2, \n",
    "    stratify=df['label'], \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {len(train_df)}\")\n",
    "print(f\"Validation set size: {len(val_df)}\")\n",
    "\n",
    "# Convert to HuggingFace datasets\n",
    "train_dataset = Dataset.from_pandas(train_df.reset_index(drop=True))\n",
    "val_dataset = Dataset.from_pandas(val_df.reset_index(drop=True))\n",
    "\n",
    "print(\"âœ… Train-validation split completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "db813238-ef25-4283-894e-514bef990402",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer: xlm-roberta-base\n",
      "âœ… Tokenizer loaded successfully!\n",
      "Tokenizer vocab size: 250002\n"
     ]
    }
   ],
   "source": [
    "# âœ… Step 5: Initialize tokenizer\n",
    "from transformers import AutoTokenizer\n",
    "model_name = \"xlm-roberta-base\"\n",
    "print(f\"Loading tokenizer: {model_name}\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "print(\"âœ… Tokenizer loaded successfully!\")\n",
    "print(f\"Tokenizer vocab size: {tokenizer.vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f0a1aa92-1e99-4f27-9de3-2fefec2b827c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Tokenization function defined!\n"
     ]
    }
   ],
   "source": [
    "# âœ… Step 6: Tokenization function\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples['text'], \n",
    "        truncation=True, \n",
    "        padding=False,  # Padding will be handled by data collator\n",
    "        max_length=512\n",
    "    )\n",
    "\n",
    "print(\"âœ… Tokenization function defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "beaed8af-50d0-4a92-8a38-be9ba531e4ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing training dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3e260be74564c87921d9fae2400727a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/22300 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Training dataset tokenized!\n",
      "Training dataset features: {'text': Value(dtype='string', id=None), 'label': Value(dtype='int32', id=None), 'input_ids': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None), 'attention_mask': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None)}\n"
     ]
    }
   ],
   "source": [
    "# Apply tokenization to training dataset\n",
    "print(\"Tokenizing training dataset...\")\n",
    "train_dataset = Dataset.from_pandas(train_df.reset_index(drop=True))\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "print(\"âœ… Training dataset tokenized!\")\n",
    "print(f\"Training dataset features: {train_dataset.features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "28cfde3b-99dd-4050-89a2-a0f8a28fcb0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing validation dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79d5b0292a524bb685f8098239faa543",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5575 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Validation dataset tokenized!\n",
      "Validation dataset features: {'text': Value(dtype='string', id=None), 'label': Value(dtype='int32', id=None), 'input_ids': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None), 'attention_mask': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None)}\n"
     ]
    }
   ],
   "source": [
    "# Apply tokenization to validation dataset\n",
    "val_dataset = Dataset.from_pandas(val_df.reset_index(drop=True))\n",
    "print(\"Tokenizing validation dataset...\")\n",
    "val_dataset = val_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "print(\"âœ… Validation dataset tokenized!\")\n",
    "print(f\"Validation dataset features: {val_dataset.features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2fc37762-9c79-4d14-b298-def2fb832152",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of labels: 2\n",
      "Loading model: xlm-roberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Model loaded successfully!\n",
      "Model device: cpu\n"
     ]
    }
   ],
   "source": [
    "# âœ… Step 7: Initialize model\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "num_labels = len(df['label'].unique())\n",
    "print(f\"Number of labels: {num_labels}\")\n",
    "\n",
    "print(f\"Loading model: {model_name}\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=num_labels,\n",
    "    problem_type=\"single_label_classification\"\n",
    ")\n",
    "\n",
    "print(\"âœ… Model loaded successfully!\")\n",
    "print(f\"Model device: {next(model.parameters()).device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b9ab610f-128c-45ba-a8f7-5caed792c081",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Final Setup Check:\n",
      "âœ… Dataset shape: (27875, 2)\n",
      "âœ… Number of labels: 2\n",
      "âœ… Training samples: 22300\n",
      "âœ… Validation samples: 5575\n",
      "âœ… Tokenizer loaded: True\n",
      "âœ… Model loaded: True\n",
      "âœ… Sample tokenization works: 33 tokens\n",
      "\n",
      "ğŸ‰ All setup complete! Ready for training configuration.\n"
     ]
    }
   ],
   "source": [
    "# Final check - make sure everything is loaded properly\n",
    "print(\"ğŸ” Final Setup Check:\")\n",
    "print(f\"âœ… Dataset shape: {df.shape}\")\n",
    "print(f\"âœ… Number of labels: {num_labels}\")\n",
    "print(f\"âœ… Training samples: {len(train_dataset)}\")\n",
    "print(f\"âœ… Validation samples: {len(val_dataset)}\")\n",
    "print(f\"âœ… Tokenizer loaded: {tokenizer is not None}\")\n",
    "print(f\"âœ… Model loaded: {model is not None}\")\n",
    "\n",
    "# Test tokenization on a sample\n",
    "sample_text = train_df.iloc[0]['text']\n",
    "sample_tokens = tokenizer(sample_text, truncation=True, max_length=512)\n",
    "print(f\"âœ… Sample tokenization works: {len(sample_tokens['input_ids'])} tokens\")\n",
    "\n",
    "print(\"\\nğŸ‰ All setup complete! Ready for training configuration.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0f449b54-1ed4-4af8-be32-9aaaade6d11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    accuracy = (predictions == labels).mean()\n",
    "    return {\"accuracy\": accuracy}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "64dc24c6-d486-4412-9bbc-aa3d08d2cf95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Data collator created successfully!\n"
     ]
    }
   ],
   "source": [
    "# Add this import\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "# Create data collator\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "print(\"âœ… Data collator created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "51220b64-2e19-4855-a0f7-9520d0f3661e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done succesfully\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "import torch\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"epoch\",        # Evaluate every epoch\n",
    "    save_strategy=\"epoch\",        # Save every epoch\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,  # Reduced from 16 to avoid memory issues\n",
    "    per_device_eval_batch_size=8,   # Reduced from 16 to avoid memory issues\n",
    "    num_train_epochs=4,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=100,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_accuracy\",\n",
    "    greater_is_better=True,\n",
    "    save_total_limit=2,\n",
    "    seed=42,\n",
    "    dataloader_pin_memory=False,\n",
    "    report_to=None,\n",
    "    fp16=True if torch.cuda.is_available() else False  # Enable mixed precision if GPU available\n",
    ")\n",
    "print(\"Done succesfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bd78859a-dd40-46d8-ad00-072428be2e6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking training setup...\n",
      "Model device: cpu\n",
      "Available GPU: False\n"
     ]
    }
   ],
   "source": [
    "# Add debugging information\n",
    "print(\"Checking training setup...\")\n",
    "# Check if CUDA is available and move model to GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Example with dummy data - replace with your actual data\n",
    "# Assuming you have features (X) and labels (y)\n",
    "X = torch.randn(1000, 10)  # 1000 samples, 10 features\n",
    "y = torch.randint(0, 2, (1000,))  # Binary classification\n",
    "\n",
    "# Create dataset and dataloader\n",
    "dataset = TensorDataset(X, y)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "# Also move your data to GPU during training\n",
    "for batch in dataloader:\n",
    "    inputs, labels = batch\n",
    "    inputs = inputs.to(device)\n",
    "    labels = labels.to(device)\n",
    "    # ... rest of training loop\n",
    "print(f\"Model device: {next(model.parameters()).device}\")\n",
    "print(f\"Available GPU: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "749c8e41-9c9b-4b05-923b-683516f9bcea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing data collator with padding to max_length=512...\n",
      "âœ… Data collator working correctly!\n",
      "Batch keys: dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'labels'])\n",
      "Input IDs shape: torch.Size([2, 512])\n",
      "Attention mask shape: torch.Size([2, 512])\n",
      "Labels shape: torch.Size([2])\n",
      "\n",
      "Sequence lengths in dataset: [10, 10]\n",
      "Min length: 10\n",
      "Max length: 10\n",
      "Average length: 10.0\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "import torch\n",
    "\n",
    "# 1. Initialize tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# 2. Set pad token if missing\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    print(f\"Set pad token to EOS token: {tokenizer.pad_token}\")\n",
    "\n",
    "# 3. Tokenize your dataset with truncation and max_length=512\n",
    "train_dataset = [\n",
    "    tokenizer(\"Hello, this is a test sentence.\", truncation=True, max_length=512),\n",
    "    tokenizer(\"Another example sentence that might be longer.\", truncation=True, max_length=512)\n",
    "]\n",
    "\n",
    "# Add dummy labels (replace with your actual labels)\n",
    "for i, sample in enumerate(train_dataset):\n",
    "    sample[\"label\"] = i % 2\n",
    "\n",
    "# 4. Create data collator that pads to max_length=512 exactly\n",
    "data_collator = DataCollatorWithPadding(\n",
    "    tokenizer=tokenizer,\n",
    "    padding=\"max_length\",   # pad sequences to max_length\n",
    "    max_length=512,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "# 5. Test data collator\n",
    "print(\"Testing data collator with padding to max_length=512...\")\n",
    "try:\n",
    "    batch = data_collator(train_dataset)\n",
    "    print(\"âœ… Data collator working correctly!\")\n",
    "    print(f\"Batch keys: {batch.keys()}\")\n",
    "    print(f\"Input IDs shape: {batch['input_ids'].shape}\")          # Should be (2, 512)\n",
    "    print(f\"Attention mask shape: {batch['attention_mask'].shape}\")# Should be (2, 512)\n",
    "    print(f\"Labels shape: {batch['labels'].shape}\")                # Should be (2,)\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Data collator error: {e}\")\n",
    "\n",
    "# 6. Check sequence lengths (should all be 512 now)\n",
    "lengths = [len(sample['input_ids']) for sample in train_dataset]\n",
    "print(f\"\\nSequence lengths in dataset: {lengths}\")\n",
    "print(f\"Min length: {min(lengths)}\")\n",
    "print(f\"Max length: {max(lengths)}\")\n",
    "print(f\"Average length: {sum(lengths)/len(lengths):.1f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cb88f7be-fa93-4b76-bc58-3720fdc5ad9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accelerator state reset successfully\n"
     ]
    }
   ],
   "source": [
    "# Add this FIRST, before any other imports\n",
    "import os\n",
    "import gc\n",
    "import torch\n",
    "\n",
    "# Clear environment variables that might cause issues\n",
    "os.environ.pop(\"ACCELERATE_USE_CPU\", None)\n",
    "os.environ.pop(\"CUDA_VISIBLE_DEVICES\", None)\n",
    "\n",
    "# Clear GPU memory\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.synchronize()\n",
    "gc.collect()\n",
    "\n",
    "# Reset accelerator state\n",
    "try:\n",
    "    from accelerate.state import AcceleratorState\n",
    "    AcceleratorState._reset_state()\n",
    "    print(\"Accelerator state reset successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not reset accelerator state: {e}\")\n",
    "\n",
    "# Now import your other libraries\n",
    "from transformers import Trainer\n",
    "# ... rest of your imports\n",
    "from transformers import Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    processing_class=tokenizer,  # Changed from tokenizer to processing_class\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c7736287-56bd-4955-963b-75b10705f0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_latest_checkpoint(output_dir):\n",
    "    \"\"\"Find the latest checkpoint in the output directory\"\"\"\n",
    "    checkpoint_pattern = os.path.join(output_dir, \"checkpoint-*\")\n",
    "    checkpoints = glob.glob(checkpoint_pattern)\n",
    "    if checkpoints:\n",
    "        # Sort by checkpoint number\n",
    "        checkpoints.sort(key=lambda x: int(x.split('-')[-1]))\n",
    "        latest_checkpoint = checkpoints[-1]\n",
    "        print(f\"Found latest checkpoint: {latest_checkpoint}\")\n",
    "        return latest_checkpoint\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701d3ea1-c58f-4481-9177-65e275a5b78b",
   "metadata": {},
   "source": [
    "ğŸ‹ï¸ Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "faf7a6ba-b977-46e5-938f-900e82b2d254",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸŒ Multilingual Offensive Speech Detection Training\n",
      "âœ… All datasets loaded successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64a1535907c54d83b294ebb33cb4e199",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/33598 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b31c786ce9ec44159575f8fa58cf3f4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7780 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Dataset debugging:\n",
      "  label: <class 'torch.Tensor'> - 0\n",
      "  input_ids: <class 'torch.Tensor'> - length 15\n",
      "  attention_mask: <class 'torch.Tensor'> - length 15\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10500' max='10500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10500/10500 42:44:35, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.415300</td>\n",
       "      <td>0.759530</td>\n",
       "      <td>0.779820</td>\n",
       "      <td>0.792382</td>\n",
       "      <td>0.779820</td>\n",
       "      <td>0.782524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.035200</td>\n",
       "      <td>0.656911</td>\n",
       "      <td>0.805656</td>\n",
       "      <td>0.814518</td>\n",
       "      <td>0.805656</td>\n",
       "      <td>0.808257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.974900</td>\n",
       "      <td>0.650470</td>\n",
       "      <td>0.771851</td>\n",
       "      <td>0.827395</td>\n",
       "      <td>0.771851</td>\n",
       "      <td>0.783951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.036600</td>\n",
       "      <td>0.651813</td>\n",
       "      <td>0.801799</td>\n",
       "      <td>0.830168</td>\n",
       "      <td>0.801799</td>\n",
       "      <td>0.809189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.823700</td>\n",
       "      <td>0.697965</td>\n",
       "      <td>0.811054</td>\n",
       "      <td>0.830382</td>\n",
       "      <td>0.811054</td>\n",
       "      <td>0.816513</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Training completed and model saved!\n",
      "\n",
      "ğŸ“Š Final Evaluation:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='487' max='487' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [487/487 18:22]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  eval_loss: 0.6980\n",
      "  eval_accuracy: 0.8111\n",
      "  eval_precision: 0.8304\n",
      "  eval_recall: 0.8111\n",
      "  eval_f1: 0.8165\n",
      "  eval_runtime: 1104.6624\n",
      "  eval_samples_per_second: 7.0430\n",
      "  eval_steps_per_second: 0.4410\n",
      "  epoch: 5.0000\n",
      "ğŸ‰ Done!\n"
     ]
    }
   ],
   "source": [
    "# âœ… Required imports\n",
    "import os\n",
    "import glob\n",
    "import torch\n",
    "import gc\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from accelerate import Accelerator\n",
    "from transformers import (\n",
    "    Trainer, \n",
    "    TrainingArguments,\n",
    "    AutoTokenizer, \n",
    "    AutoModelForSequenceClassification,\n",
    "    DataCollatorWithPadding,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from datasets import Dataset\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# âœ… Checkpoint finder\n",
    "def find_latest_checkpoint(output_dir):\n",
    "    checkpoint_pattern = os.path.join(output_dir, \"checkpoint-*\")\n",
    "    checkpoints = glob.glob(checkpoint_pattern)\n",
    "    if checkpoints:\n",
    "        checkpoints.sort(key=lambda x: int(x.split('-')[-1]))\n",
    "        return checkpoints[-1]\n",
    "    return None\n",
    "\n",
    "# âœ… Clear memory\n",
    "def clear_memory():\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()\n",
    "    gc.collect()\n",
    "\n",
    "\n",
    "# âœ… Trainer creator\n",
    "# âœ… Custom trainer with class weights\n",
    "class WeightedTrainer(Trainer):\n",
    "    def __init__(self, class_weights=None, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.class_weights = class_weights\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get('logits')\n",
    "\n",
    "        if self.class_weights is not None:\n",
    "            weight_tensor = torch.tensor(list(self.class_weights.values()), \n",
    "                                         dtype=torch.float32, device=logits.device)\n",
    "            loss_fct = torch.nn.CrossEntropyLoss(weight=weight_tensor)\n",
    "        else:\n",
    "            loss_fct = torch.nn.CrossEntropyLoss()\n",
    "        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "# âœ… Trainer creator (fix parameter name tokenizer)\n",
    "def create_trainer(model, training_args, train_dataset, val_dataset, tokenizer, data_collator, compute_metrics, class_weights=None):\n",
    "    return WeightedTrainer(\n",
    "        class_weights=class_weights,\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        tokenizer=tokenizer,                 # Fixed from processing_class=tokenizer\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    "    )\n",
    "\n",
    "# âœ… Metrics\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    from sklearn.metrics import precision_recall_fscore_support\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='weighted', zero_division=0)\n",
    "    return {\n",
    "        \"accuracy\": (predictions == labels).mean(),\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1\n",
    "    }\n",
    "\n",
    "# âœ… Label encoder\n",
    "def encode_labels(df, label_mapping=None):\n",
    "    df = df.copy()\n",
    "    df['label'] = df['label'].astype(str).str.strip().str.lower()\n",
    "    unique_labels = list(df['label'].unique())\n",
    "    unique_labels.sort()\n",
    "    if label_mapping is None:\n",
    "        label_mapping = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "    df['label'] = df['label'].map(label_mapping)\n",
    "    df = df.dropna(subset=['label'])\n",
    "    df['label'] = df['label'].astype(int)\n",
    "    return df, label_mapping\n",
    "\n",
    "# âœ… Data loader\n",
    "def load_and_preprocess_data():\n",
    "    try:\n",
    "        tamil_train = pd.read_csv(\"tamil_offensive_speech_train.csv\")[[\"comment\", \"label\"]].rename(columns={'comment': 'text'})\n",
    "        tamil_val = pd.read_csv(\"tamil_offensive_speech_val.csv\")[[\"comment\", \"label\"]].rename(columns={'comment': 'text'})\n",
    "        tamil_train[\"lang\"] = \"ta\"\n",
    "        tamil_val[\"lang\"] = \"ta\"\n",
    "\n",
    "        hindi_train = pd.read_csv(\"Hatespeech-Hindi_Train.csv\")[[\"Post\", \"Labels Set\"]].rename(columns={'Post': 'text', 'Labels Set': 'label'})\n",
    "        hindi_val = pd.read_csv(\"Hatespeech-Hindi_Valid.csv\")[[\"Post\", \"Labels Set\"]].rename(columns={'Post': 'text', 'Labels Set': 'label'})\n",
    "        hindi_train[\"lang\"] = \"hi\"\n",
    "        hindi_val[\"lang\"] = \"hi\"\n",
    "\n",
    "        print(\"âœ… All datasets loaded successfully\")\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"âŒ Error loading datasets: {e}\")\n",
    "        return None, None, None\n",
    "\n",
    "    train_df = pd.concat([tamil_train, hindi_train], ignore_index=True).dropna().reset_index(drop=True)\n",
    "    val_df = pd.concat([tamil_val, hindi_val], ignore_index=True).dropna().reset_index(drop=True)\n",
    "    train_df, label_mapping = encode_labels(train_df)\n",
    "    val_df, _ = encode_labels(val_df, label_mapping)\n",
    "\n",
    "    class_weights = compute_class_weight('balanced', classes=np.unique(train_df['label']), y=train_df['label'])\n",
    "    class_weight_dict = {i: weight for i, weight in enumerate(class_weights)}\n",
    "    return train_df, val_df, len(set(train_df['label'].unique())), class_weight_dict\n",
    "\n",
    "# âœ… Main training logic\n",
    "def run_training(model, training_args, train_dataset, val_dataset, tokenizer, data_collator, compute_metrics, class_weights=None):\n",
    "    print(f\"ğŸ” Dataset debugging:\")\n",
    "    sample_item = train_dataset[0]\n",
    "    for key, value in sample_item.items():\n",
    "        if key == 'label':\n",
    "            display_val = value\n",
    "        else:\n",
    "            display_val = f\"length {len(value)}\" if hasattr(value, '__len__') else \"length N/A\"\n",
    "        print(f\"  {key}: {type(value)} - {display_val}\")\n",
    "\n",
    "    latest_checkpoint = find_latest_checkpoint(\"./results\")\n",
    "    if latest_checkpoint and not os.path.exists(os.path.join(latest_checkpoint, \"trainer_state.json\")):\n",
    "        latest_checkpoint = None\n",
    "\n",
    "    trainer = create_trainer(model, training_args, train_dataset, val_dataset, tokenizer, data_collator, compute_metrics, class_weights)\n",
    "    retry_count, max_retries = 0, 3\n",
    "    while retry_count < max_retries:\n",
    "        try:\n",
    "            if latest_checkpoint and retry_count == 0:\n",
    "                trainer.train(resume_from_checkpoint=latest_checkpoint)\n",
    "            else:\n",
    "                trainer.train()\n",
    "            trainer.save_model(\"./final_model\")\n",
    "            tokenizer.save_pretrained(\"./final_model\")\n",
    "            print(\"âœ… Training completed and model saved!\")\n",
    "            return trainer\n",
    "        except RuntimeError as e:\n",
    "            if \"out of memory\" in str(e).lower():\n",
    "                retry_count += 1\n",
    "                training_args.per_device_train_batch_size = max(1, training_args.per_device_train_batch_size // 2)\n",
    "                training_args.per_device_eval_batch_size = max(1, training_args.per_device_eval_batch_size // 2)\n",
    "                del trainer\n",
    "                clear_memory()\n",
    "                time.sleep(5)\n",
    "                trainer = create_trainer(model, training_args, train_dataset, val_dataset, tokenizer, data_collator, compute_metrics, class_weights)\n",
    "                print(f\"Retrying with reduced batch sizes. Attempt: {retry_count}\")\n",
    "            else:\n",
    "                raise\n",
    "        except KeyboardInterrupt:\n",
    "            print(\"â¹ï¸ Training interrupted. Progress saved.\")\n",
    "            break\n",
    "    return None\n",
    "\n",
    "# âœ… Main\n",
    "def main():\n",
    "    print(\"ğŸŒ Multilingual Offensive Speech Detection Training\")\n",
    "    train_df, val_df, num_labels, class_weights = load_and_preprocess_data()\n",
    "    if train_df is None:\n",
    "        return\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-base\")\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\"xlm-roberta-base\", num_labels=num_labels)\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./checkpoints\",\n",
    "        save_strategy=\"epoch\",          # save after each epoch\n",
    "        eval_strategy=\"epoch\",    # evaluate after each epoch\n",
    "        save_total_limit=3,\n",
    "        load_best_model_at_end=True,\n",
    "        greater_is_better=True,\n",
    "        num_train_epochs=5,\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=16,\n",
    "        warmup_steps=500,\n",
    "        weight_decay=0.01,\n",
    "        learning_rate=2e-5,\n",
    "        logging_dir='./logs',\n",
    "        logging_steps=100,\n",
    "        metric_for_best_model=\"f1\",\n",
    "        report_to=[],\n",
    "        dataloader_pin_memory=False,\n",
    "        fp16=torch.cuda.is_available(),\n",
    "        dataloader_num_workers=4 if torch.cuda.is_available() else 0,\n",
    "        remove_unused_columns=True,\n",
    ")\n",
    "\n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "    def tokenize_function(examples):\n",
    "        return tokenizer(examples['text'], truncation=True, padding=False, max_length=256)\n",
    "\n",
    "    train_dataset = Dataset.from_pandas(train_df).map(tokenize_function, batched=True, remove_columns=[\"text\", \"lang\"])\n",
    "    val_dataset = Dataset.from_pandas(val_df).map(tokenize_function, batched=True, remove_columns=[\"text\", \"lang\"])\n",
    "    train_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "    val_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "\n",
    "    trainer = run_training(model, training_args, train_dataset, val_dataset, tokenizer, data_collator, compute_metrics, class_weights)\n",
    "\n",
    "    if trainer:\n",
    "        print(\"\\nğŸ“Š Final Evaluation:\")\n",
    "        results = trainer.evaluate()\n",
    "        for key, value in results.items():\n",
    "            print(f\"  {key}: {value:.4f}\")\n",
    "\n",
    "    print(\"ğŸ‰ Done!\")\n",
    "\n",
    "# âœ… Run\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0da5932-fc8a-4069-90fb-bec5b9e64e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_prediction(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "        predicted_class = torch.argmax(predictions, dim=-1).item()\n",
    "    return predicted_class, predictions[0].tolist()\n",
    "# Example usage - uncomment to test\n",
    "# test_text = \"This is a sample text\"\n",
    "# pred_class, confidence = test_prediction(test_text)\n",
    "# print(f\"Predicted class: {pred_class}, Confidence: {confidence}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683906fa-0cea-4378-b574-e68a47236047",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "predictions = trainer.predict(val_dataset)\n",
    "pred_labels = predictions.predictions.argmax(axis=1)\n",
    "\n",
    "print(classification_report(val_labels, pred_labels, target_names=label_map.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f66796-d24f-4b04-a257-4f4721a8efaf",
   "metadata": {},
   "source": [
    "âœ… Conclusion\n",
    "- Multilingual model trained using Hindi and Tamil data\n",
    "- Powered by XLM-RoBERTa for cross-lingual learning\n",
    "- Ready to deploy as REST API or chatbot"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
