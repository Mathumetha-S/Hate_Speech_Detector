{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a70cfa28-551f-4a93-9139-3a9f066d93c2",
   "metadata": {},
   "source": [
    " Hate Speech Detector (Hindi and Tamil) \n",
    " To detect hate/offensive speech in Hindi and Tamil using `XLM-RoBERTa'\n",
    " üîç Use Case\n",
    "- Moderate content on multilingual Indian social platforms (Facebook, ShareChat)\n",
    "- Prevent caste, religious, and political hate speech\n",
    "- Useful for detecting hate in code-mixed regional content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df922e5e-95b8-4bec-bf74-71869fb3d688",
   "metadata": {},
   "outputs": [],
   "source": [
    "!jupyter nbextension enable --py widgetsnbextension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc2f417-c5e6-4830-85cb-c7d55b01328a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --force-reinstall sympy==1.13.1\n",
    "print(\"Success\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d80acb91-bfc5-4c84-b2a3-ef2b421bfe45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SymPy version: 1.13.1\n",
      "mpmath version: 1.3.0\n",
      "Success - packages imported correctly!\n"
     ]
    }
   ],
   "source": [
    "import sympy\n",
    "import mpmath\n",
    "print(f\"SymPy version: {sympy.__version__}\")\n",
    "print(f\"mpmath version: {mpmath.__version__}\")\n",
    "print(\"Success - packages imported correctly!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6814d807-622f-4134-9812-e83ca9d29eff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in c:\\users\\mathu\\appdata\\roaming\\python\\python312\\site-packages (4.52.4)\n",
      "Requirement already satisfied: datasets in c:\\users\\mathu\\appdata\\roaming\\python\\python312\\site-packages (3.6.0)\n",
      "Requirement already satisfied: nltk in c:\\programdata\\anaconda3\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\programdata\\anaconda3\\lib\\site-packages (1.5.1)\n",
      "Requirement already satisfied: pandas in c:\\programdata\\anaconda3\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in c:\\users\\mathu\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (0.32.3)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\mathu\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\mathu\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from datasets) (16.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: xxhash in c:\\users\\mathu\\appdata\\roaming\\python\\python312\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\mathu\\appdata\\roaming\\python\\python312\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2024.6.1)\n",
      "Requirement already satisfied: click in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\programdata\\anaconda3\\lib\\site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.10.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.11.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (2024.8.30)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.11.0)\n",
      "Success\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers datasets nltk scikit-learn pandas\n",
    "print(\"Success\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e5f862-c723-44fd-a12f-7d294242129c",
   "metadata": {},
   "source": [
    "üì• Load and Combine Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4f026cf-3bc9-4899-951b-c75d4aba6087",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: numpy<2 in c:\\programdata\\anaconda3\\lib\\site-packages (1.26.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install \"numpy<2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6bc37b91-eb39-44b3-86fd-be19aaeb0425",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Both packages working!\n",
      "NumPy version: 1.26.4\n",
      "Pandas version: 2.2.2\n",
      "NumPy location: C:\\ProgramData\\anaconda3\\Lib\\site-packages\\numpy\\__init__.py\n",
      "Pandas location: C:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\__init__.py\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "print(\"‚úì Both packages working!\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"NumPy location: {np.__file__}\")\n",
    "print(f\"Pandas location: {pd.__file__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fdc076bf-49aa-463a-b583-aa27fb5992af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy version: 1.26.4\n",
      "NumPy location: C:\\ProgramData\\anaconda3\\Lib\\site-packages\\numpy\\__init__.py\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"NumPy location: {np.__file__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "910a97ec-ba1e-46e1-ac43-4851464e0108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Unique ID', 'Post', 'Labels Set'], dtype='object')\n",
      "Index(['Unique ID', 'Post', 'Labels Set'], dtype='object')\n",
      "Read successfully\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "hindi_train = pd.read_csv(\"Hatespeech-Hindi_Train.csv\")[[\"Post\", \"Labels Set\"]]\n",
    "hindi_val = pd.read_csv(\"Hatespeech-Hindi_Valid.csv\")[[\"Post\", \"Labels Set\"]]\n",
    "print(pd.read_csv(\"Hatespeech-Hindi_Train.csv\").columns)\n",
    "print(pd.read_csv(\"Hatespeech-Hindi_Valid.csv\").columns)\n",
    "print(\"Read successfully\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0a6d183f-c34e-47de-93b8-876cfcc7360a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Renamed Successfully\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>lang</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>omg that bgm make me goosebumb...</td>\n",
       "      <td>0</td>\n",
       "      <td>ta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>neraya neraya neraya neraya neraya neraya.</td>\n",
       "      <td>0</td>\n",
       "      <td>ta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>thalaivar mersal look .semma massss thalaiva ....</td>\n",
       "      <td>0</td>\n",
       "      <td>ta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>paaaa... repeat mode.... adra adra adraaaaa......</td>\n",
       "      <td>0</td>\n",
       "      <td>ta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>epaa ena panaporam... sweet sapade poram... aw...</td>\n",
       "      <td>0</td>\n",
       "      <td>ta</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text label lang\n",
       "0                  omg that bgm make me goosebumb...     0   ta\n",
       "1         neraya neraya neraya neraya neraya neraya.     0   ta\n",
       "2  thalaivar mersal look .semma massss thalaiva ....     0   ta\n",
       "3  paaaa... repeat mode.... adra adra adraaaaa......     0   ta\n",
       "4  epaa ena panaporam... sweet sapade poram... aw...     0   ta"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Tamil files\n",
    "tamil_train = pd.read_csv(\"tamil_offensive_speech_train.csv\")[[\"comment\", \"label\"]]\n",
    "tamil_val = pd.read_csv(\"tamil_offensive_speech_val.csv\")[[\"comment\", \"label\"]]\n",
    "\n",
    "# Rename 'comment' column to 'text' to be consistent with Hindi dataset\n",
    "tamil_train = tamil_train.rename(columns={'comment': 'text'})\n",
    "tamil_val = tamil_val.rename(columns={'comment': 'text'})\n",
    "tamil_train[\"lang\"] = \"ta\"\n",
    "tamil_val[\"lang\"] = \"ta\"\n",
    "\n",
    "# Hindi files\n",
    "hindi_train = pd.read_csv(\"Hatespeech-Hindi_Train.csv\")[[\"Post\", \"Labels Set\"]]\n",
    "hindi_val = pd.read_csv(\"Hatespeech-Hindi_Valid.csv\")[[\"Post\", \"Labels Set\"]]\n",
    "\n",
    "# Rename columns to be consistent\n",
    "hindi_train = hindi_train.rename(columns={'Post': 'text', 'Labels Set': 'label'})\n",
    "hindi_val = hindi_val.rename(columns={'Post': 'text', 'Labels Set': 'label'})\n",
    "print(\"Renamed Successfully\")\n",
    "\n",
    "hindi_train[\"lang\"] = \"hi\"\n",
    "hindi_val[\"lang\"] = \"hi\"\n",
    "\n",
    "# Combine into df\n",
    "df = pd.concat([tamil_train, tamil_val, hindi_train, hindi_val], ignore_index=True)\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# View data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49356e19-e6d3-4c47-b346-10cd6e50ca65",
   "metadata": {},
   "source": [
    "üßπ Clean and Preprocess Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "28491c3f-526b-41ff-8a32-020dada5bf61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['label', 'text'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load your dataset\n",
    "df = pd.read_csv(\"tamil_offensive_speech_train.csv\")\n",
    "\n",
    "# Rename 'comment' to 'text'\n",
    "df = df.rename(columns={'comment': 'text'})\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "85238ded-402a-4a9b-961b-f82e23b00507",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows with missing labels: 0\n",
      "Final sample:\n",
      "   label                                               text\n",
      "0      0                     omg that bgm make me goosebumb\n",
      "1      0          neraya neraya neraya neraya neraya neraya\n",
      "2      0  thalaivar mersal look semma massss thalaiva th...\n",
      "3      0  paaaa repeat mode adra adra adraaaaa vera leve...\n",
      "4      0  epaa ena panaporam sweet sapade poram awesome ...\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "def clean_text(text):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "# Assuming df is your dataframe and has columns 'text' and 'label'\n",
    "\n",
    "# Clean text column\n",
    "df['text'] = df['text'].apply(clean_text)\n",
    "\n",
    "def map_label(label):\n",
    "    if pd.isna(label):\n",
    "        return None\n",
    "    label = str(label).lower()\n",
    "\n",
    "    if label in ['0', 'non-hostile', 'normal']:\n",
    "        return 0\n",
    "    elif 'hate' in label:\n",
    "        return 2\n",
    "    elif any(word in label for word in ['offensive', 'defamation', 'fake']):\n",
    "        return 1\n",
    "    elif label in ['1']:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0  # default to normal if unclear\n",
    "\n",
    "df['label'] = df['label'].apply(map_label)\n",
    "\n",
    "print(f\"Rows with missing labels: {df['label'].isna().sum()}\")\n",
    "df.dropna(subset=['label'], inplace=True)\n",
    "df['label'] = df['label'].astype(int)\n",
    "\n",
    "# Drop rows with empty or missing text\n",
    "df = df[df['text'].str.strip() != '']\n",
    "\n",
    "print(\"Final sample:\")\n",
    "print(df.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee647546-543f-48be-a173-7de05e4f2962",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    df['text'], df['label'], test_size=0.2, stratify=df['label'], random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5ee5c108-7449-4cf0-96aa-d98ef916df57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "0    21224\n",
      "1     6649\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df['label'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "593548ac-8cfd-401c-90ab-cff8d780e4dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 22298\n",
      "Validation size: 5575\n",
      "Train label distribution:\n",
      " label\n",
      "0    16979\n",
      "1     5319\n",
      "Name: count, dtype: int64\n",
      "Validation label distribution:\n",
      " label\n",
      "0    4245\n",
      "1    1330\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Train size:\", len(train_texts))\n",
    "print(\"Validation size:\", len(val_texts))\n",
    "print(\"Train label distribution:\\n\", pd.Series(train_labels).value_counts())\n",
    "print(\"Validation label distribution:\\n\", pd.Series(val_labels).value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4ec89f-0542-4bf8-9acd-df4ae21bf94f",
   "metadata": {},
   "source": [
    "ü§ñ Tokenize using XLM-RoBERTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6fbc3bd4-7a21-4c66-8ee8-412f177f00f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torch in c:\\users\\mathu\\appdata\\roaming\\python\\python312\\site-packages (2.7.0)\n",
      "Requirement already satisfied: torchvision in c:\\users\\mathu\\appdata\\roaming\\python\\python312\\site-packages (0.22.0)\n",
      "Requirement already satisfied: torchaudio in c:\\users\\mathu\\appdata\\roaming\\python\\python312\\site-packages (2.7.0)\n",
      "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\mathu\\appdata\\roaming\\python\\python312\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (75.1.0)\n",
      "Requirement already satisfied: numpy in c:\\programdata\\anaconda3\\lib\\site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from torchvision) (10.4.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\mathu\\appdata\\roaming\\python\\python312\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5fe0d63f-aa61-42c9-81d5-a605bd13bc78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.52.4\n",
      "2.7.0+cpu\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "print(transformers.__version__)\n",
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c559059e-6960-47d9-b94d-f597fbc6c78a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\python.exe\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "93cb6167-77ee-4b82-a5cc-44ceaa0dd6c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Skipping torch as it is not installed.\n",
      "WARNING: Skipping torchvision as it is not installed.\n",
      "WARNING: Skipping torchaudio as it is not installed.\n",
      "WARNING: Skipping transformers as it is not installed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files removed: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: No matching packages\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall torch torchvision torchaudio transformers -y\n",
    "!pip cache purge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "72042b28-e54d-403a-b9b5-3ba96ec157f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torch in c:\\users\\mathu\\appdata\\roaming\\python\\python312\\site-packages (2.7.0)\n",
      "Collecting torchvision\n",
      "  Downloading torchvision-0.22.0-cp312-cp312-win_amd64.whl.metadata (6.3 kB)\n",
      "Collecting torchaudio\n",
      "  Downloading torchaudio-2.7.0-cp312-cp312-win_amd64.whl.metadata (6.7 kB)\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.52.4-py3-none-any.whl.metadata (38 kB)\n",
      "Requirement already satisfied: accelerate in c:\\users\\mathu\\appdata\\roaming\\python\\python312\\site-packages (1.7.0)\n",
      "Requirement already satisfied: datasets in c:\\users\\mathu\\appdata\\roaming\\python\\python312\\site-packages (3.6.0)\n",
      "Requirement already satisfied: pandas in c:\\programdata\\anaconda3\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: scikit-learn in c:\\programdata\\anaconda3\\lib\\site-packages (1.5.1)\n",
      "Requirement already satisfied: numpy in c:\\programdata\\anaconda3\\lib\\site-packages (1.26.4)\n",
      "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\mathu\\appdata\\roaming\\python\\python312\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (75.1.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from torchvision) (10.4.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in c:\\users\\mathu\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (0.32.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\mathu\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\mathu\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: psutil in c:\\programdata\\anaconda3\\lib\\site-packages (from accelerate) (5.9.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from datasets) (16.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: xxhash in c:\\users\\mathu\\appdata\\roaming\\python\\python312\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\mathu\\appdata\\roaming\\python\\python312\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\programdata\\anaconda3\\lib\\site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.10.5)\n",
      "Requirement already satisfied: six>=1.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (2024.8.30)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\mathu\\appdata\\roaming\\python\\python312\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.11.0)\n",
      "Downloading torchvision-0.22.0-cp312-cp312-win_amd64.whl (1.7 MB)\n",
      "   ---------------------------------------- 0.0/1.7 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 0.3/1.7 MB ? eta -:--:--\n",
      "   ------------------------ --------------- 1.0/1.7 MB 3.1 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 1.6/1.7 MB 2.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.7/1.7 MB 2.7 MB/s eta 0:00:00\n",
      "Downloading torchaudio-2.7.0-cp312-cp312-win_amd64.whl (2.5 MB)\n",
      "   ---------------------------------------- 0.0/2.5 MB ? eta -:--:--\n",
      "   -------- ------------------------------- 0.5/2.5 MB 2.4 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 1.0/2.5 MB 2.2 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 1.8/2.5 MB 2.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.5/2.5 MB 3.0 MB/s eta 0:00:00\n",
      "Downloading transformers-4.52.4-py3-none-any.whl (10.5 MB)\n",
      "   ---------------------------------------- 0.0/10.5 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 0.8/10.5 MB 6.7 MB/s eta 0:00:02\n",
      "   ------ --------------------------------- 1.6/10.5 MB 3.8 MB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 2.6/10.5 MB 4.1 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 3.9/10.5 MB 4.7 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 4.7/10.5 MB 4.6 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 6.0/10.5 MB 4.9 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 7.6/10.5 MB 5.2 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 8.7/10.5 MB 5.2 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 9.7/10.5 MB 5.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  10.2/10.5 MB 4.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 10.5/10.5 MB 4.8 MB/s eta 0:00:00\n",
      "Installing collected packages: torchvision, torchaudio, transformers\n",
      "Successfully installed torchaudio-2.7.0 torchvision-0.22.0 transformers-4.52.4\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision torchaudio transformers accelerate datasets pandas scikit-learn numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f11cb67d-fcc3-4f89-9d96-ab74b31bb8a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.12.7 | packaged by Anaconda, Inc. | (main, Oct  4 2024, 13:17:27) [MSC v.1929 64 bit (AMD64)]\n",
      "‚úÖ transformers - OK\n",
      "‚úÖ accelerate - OK\n",
      "‚úÖ datasets - OK\n",
      "‚úÖ torch - OK\n",
      "‚úÖ pandas - OK\n",
      "‚úÖ sklearn - OK\n",
      "‚úÖ numpy - OK\n"
     ]
    }
   ],
   "source": [
    "# Test if everything is installed correctly\n",
    "import sys\n",
    "print(\"Python version:\", sys.version)\n",
    "\n",
    "packages_to_test = [\n",
    "    'transformers', 'accelerate', 'datasets', \n",
    "    'torch', 'pandas', 'sklearn', 'numpy'\n",
    "]\n",
    "\n",
    "for package in packages_to_test:\n",
    "    try:\n",
    "        __import__(package)\n",
    "        print(f\"‚úÖ {package} - OK\")\n",
    "    except ImportError as e:\n",
    "        print(f\"‚ùå {package} - Missing: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9fc1a3a6-5f1c-436c-b258-6f0e4d5395e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformers version: 4.52.4\n",
      "Accelerate version: 1.7.0\n",
      "PyTorch version: 2.6.0+cpu\n",
      "CUDA available: False\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import accelerate\n",
    "import torch\n",
    "\n",
    "print(f\"Transformers version: {transformers.__version__}\")\n",
    "print(f\"Accelerate version: {accelerate.__version__}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "06152ee4-5f72-4446-ac37-800bf02a070f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing pandas...\n",
      "‚úì Pandas imported successfully\n",
      "Importing sklearn...\n",
      "‚úì Sklearn imported successfully\n",
      "Importing numpy...\n",
      "‚úì Numpy imported successfully\n",
      "Importing torch...\n",
      "‚úì Torch imported successfully\n",
      "Importing datasets...\n",
      "‚úì Datasets imported successfully\n",
      "Importing transformers...\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'PreTrainedModel' from 'transformers' (C:\\Users\\mathu\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 23\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m‚úì Datasets imported successfully\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImporting transformers...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     24\u001b[0m     AutoTokenizer,\n\u001b[0;32m     25\u001b[0m     AutoModelForSequenceClassification,\n\u001b[0;32m     26\u001b[0m     TrainingArguments,\n\u001b[0;32m     27\u001b[0m     Trainer,\n\u001b[0;32m     28\u001b[0m     DataCollatorWithPadding,\n\u001b[0;32m     29\u001b[0m     EarlyStoppingCallback\n\u001b[0;32m     30\u001b[0m )\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m‚úì Transformers imported successfully\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSetting environment...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\utils\\import_utils.py:2045\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   2043\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m   2044\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 2045\u001b[0m         module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_module(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module[name])\n\u001b[0;32m   2046\u001b[0m         value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[0;32m   2047\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m, \u001b[38;5;167;01mRuntimeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\utils\\import_utils.py:2075\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   2073\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m module_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m   2074\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m-> 2075\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\utils\\import_utils.py:2073\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   2071\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_module\u001b[39m(\u001b[38;5;28mself\u001b[39m, module_name: \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m   2072\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 2073\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m module_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m   2074\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   2075\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\importlib\\__init__.py:90\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m     88\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m     89\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m---> 90\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _bootstrap\u001b[38;5;241m.\u001b[39m_gcd_import(name[level:], package, level)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\trainer.py:42\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TYPE_CHECKING, Any, Callable, Optional, Union\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# Integrations must be imported before ML frameworks:\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# isort: off\u001b[39;00m\n\u001b[1;32m---> 42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mintegrations\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     43\u001b[0m     get_reporting_integration_callbacks,\n\u001b[0;32m     44\u001b[0m )\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# isort: on\u001b[39;00m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mhuggingface_hub\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mhf_hub_utils\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\utils\\import_utils.py:2045\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   2043\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m   2044\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 2045\u001b[0m         module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_module(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module[name])\n\u001b[0;32m   2046\u001b[0m         value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[0;32m   2047\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m, \u001b[38;5;167;01mRuntimeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\utils\\import_utils.py:2075\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   2073\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m module_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m   2074\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m-> 2075\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\utils\\import_utils.py:2073\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   2071\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_module\u001b[39m(\u001b[38;5;28mself\u001b[39m, module_name: \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m   2072\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 2073\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m module_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m   2074\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   2075\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\importlib\\__init__.py:90\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m     88\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m     89\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m---> 90\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _bootstrap\u001b[38;5;241m.\u001b[39m_gcd_import(name[level:], package, level)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\integrations\\integration_utils.py:37\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpackaging\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversion\u001b[39;00m\n\u001b[1;32m---> 37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PreTrainedModel, TFPreTrainedModel, TrainingArguments\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__ \u001b[38;5;28;01mas\u001b[39;00m version\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     40\u001b[0m     PushToHubMixin,\n\u001b[0;32m     41\u001b[0m     flatten_dict,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     46\u001b[0m     logging,\n\u001b[0;32m     47\u001b[0m )\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'PreTrainedModel' from 'transformers' (C:\\Users\\mathu\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\__init__.py)"
     ]
    }
   ],
   "source": [
    "# Test each import separately\n",
    "print(\"Importing pandas...\")\n",
    "import pandas as pd\n",
    "print(\"‚úì Pandas imported successfully\")\n",
    "\n",
    "print(\"Importing sklearn...\")\n",
    "from sklearn.model_selection import train_test_split\n",
    "print(\"‚úì Sklearn imported successfully\")\n",
    "\n",
    "print(\"Importing numpy...\")\n",
    "import numpy as np\n",
    "print(\"‚úì Numpy imported successfully\")\n",
    "\n",
    "print(\"Importing torch...\")\n",
    "import torch\n",
    "print(\"‚úì Torch imported successfully\")\n",
    "\n",
    "print(\"Importing datasets...\")\n",
    "from datasets import Dataset\n",
    "print(\"‚úì Datasets imported successfully\")\n",
    "\n",
    "print(\"Importing transformers...\")\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorWithPadding,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "print(\"‚úì Transformers imported successfully\")\n",
    "\n",
    "print(\"Setting environment...\")\n",
    "import os\n",
    "import glob\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "print(\"‚úì All imports completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0f83ce-3be0-4655-b6c6-778389df3034",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPLETE REPLACEMENT CODE - Use this instead of your original code\n",
    "# This replaces ALL your import and training code\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset as TorchDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "from datasets import Dataset\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set environment\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "print(\"‚úÖ All imports successful - no Trainer issues!\")\n",
    "\n",
    "class CustomDataset(TorchDataset):\n",
    "    \"\"\"Custom PyTorch Dataset for text classification\"\"\"\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "def preprocess_hindi_labels(label_str):\n",
    "    \"\"\"Convert Hindi multi-label format to binary classification\"\"\"\n",
    "    if pd.isna(label_str) or label_str == 'non-hostile':\n",
    "        return 0  # Non-hate\n",
    "    else:\n",
    "        return 1  # Hate (any form of hate/offensive/defamation)\n",
    "\n",
    "def load_and_prepare_combined_data():\n",
    "    \"\"\"Load and prepare the combined Tamil-Hindi datasets\"\"\"\n",
    "    try:\n",
    "        print(\"üìÇ Loading Tamil datasets...\")\n",
    "        # Tamil files\n",
    "        tamil_train = pd.read_csv(\"tamil_offensive_speech_train.csv\")[[\"comment\", \"label\"]]\n",
    "        tamil_val = pd.read_csv(\"tamil_offensive_speech_val.csv\")[[\"comment\", \"label\"]]\n",
    "        \n",
    "        # Rename 'comment' column to 'text' to be consistent with Hindi dataset\n",
    "        tamil_train = tamil_train.rename(columns={'comment': 'text'})\n",
    "        tamil_val = tamil_val.rename(columns={'comment': 'text'})\n",
    "        tamil_train[\"lang\"] = \"ta\"\n",
    "        tamil_val[\"lang\"] = \"ta\"\n",
    "        \n",
    "        print(f\"‚úÖ Tamil train: {len(tamil_train)} samples\")\n",
    "        print(f\"‚úÖ Tamil validation: {len(tamil_val)} samples\")\n",
    "        print(f\"üìä Tamil label distribution:\")\n",
    "        print(tamil_train['label'].value_counts())\n",
    "        \n",
    "        print(\"\\nüìÇ Loading Hindi datasets...\")\n",
    "        # Hindi files\n",
    "        hindi_train = pd.read_csv(\"Hatespeech-Hindi_Train.csv\")[[\"Post\", \"Labels Set\"]]\n",
    "        hindi_val = pd.read_csv(\"Hatespeech-Hindi_Valid.csv\")[[\"Post\", \"Labels Set\"]]\n",
    "        \n",
    "        # Rename columns to be consistent\n",
    "        hindi_train = hindi_train.rename(columns={'Post': 'text', 'Labels Set': 'label'})\n",
    "        hindi_val = hindi_val.rename(columns={'Post': 'text', 'Labels Set': 'label'})\n",
    "        \n",
    "        print(f\"‚úÖ Hindi train: {len(hindi_train)} samples\")\n",
    "        print(f\"‚úÖ Hindi validation: {len(hindi_val)} samples\")\n",
    "        print(f\"üìä Original Hindi label distribution:\")\n",
    "        print(hindi_train['label'].value_counts())\n",
    "        \n",
    "        # Process Hindi labels to binary format\n",
    "        hindi_train['label'] = hindi_train['label'].apply(preprocess_hindi_labels)\n",
    "        hindi_val['label'] = hindi_val['label'].apply(preprocess_hindi_labels)\n",
    "        \n",
    "        print(f\"üìä Processed Hindi label distribution:\")\n",
    "        print(hindi_train['label'].value_counts())\n",
    "        \n",
    "        hindi_train[\"lang\"] = \"hi\"\n",
    "        hindi_val[\"lang\"] = \"hi\"\n",
    "        \n",
    "        print(\"\\nüîÑ Combining datasets...\")\n",
    "        # Combine into df\n",
    "        df = pd.concat([tamil_train, tamil_val, hindi_train, hindi_val], ignore_index=True)\n",
    "        df.dropna(inplace=True)\n",
    "        \n",
    "        print(f\"‚úÖ Combined dataset: {len(df)} samples\")\n",
    "        print(f\"üìä Final label distribution:\")\n",
    "        print(df['label'].value_counts())\n",
    "        print(f\"üìä Language distribution:\")\n",
    "        print(df['lang'].value_counts())\n",
    "        \n",
    "        # Display sample data\n",
    "        print(f\"\\nüîç Sample data:\")\n",
    "        print(df.head())\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"‚ùå File not found: {e}\")\n",
    "        print(\"üìÅ Make sure all CSV files are in the same directory\")\n",
    "        return None\n",
    "\n",
    "def train_model(model, train_dataloader, val_dataloader, device, epochs=3, lr=2e-5):\n",
    "    \"\"\"Manual training loop - replaces Trainer\"\"\"\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    total_steps = len(train_dataloader) * epochs\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=0,\n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "    \n",
    "    model.to(device)\n",
    "    best_val_accuracy = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        print(f'\\nüìö Epoch {epoch + 1}/{epochs}')\n",
    "        print('-' * 50)\n",
    "        \n",
    "        # Training phase\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        train_predictions = []\n",
    "        train_true = []\n",
    "        \n",
    "        train_pbar = tqdm(train_dataloader, desc=\"Training\")\n",
    "        for batch in train_pbar:\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            outputs = model(input_ids=input_ids, \n",
    "                          attention_mask=attention_mask, \n",
    "                          labels=labels)\n",
    "            \n",
    "            loss = outputs.loss\n",
    "            total_train_loss += loss.item()\n",
    "            \n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "            train_predictions.extend(predictions.cpu().numpy())\n",
    "            train_true.extend(labels.cpu().numpy())\n",
    "            \n",
    "            train_pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "        \n",
    "        avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "        train_accuracy = accuracy_score(train_true, train_predictions)\n",
    "        \n",
    "        print(f'üìä Training Loss: {avg_train_loss:.4f} | Accuracy: {train_accuracy:.4f}')\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        total_eval_loss = 0\n",
    "        eval_predictions = []\n",
    "        eval_true = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            val_pbar = tqdm(val_dataloader, desc=\"Validation\")\n",
    "            for batch in val_pbar:\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "                \n",
    "                outputs = model(input_ids=input_ids, \n",
    "                              attention_mask=attention_mask, \n",
    "                              labels=labels)\n",
    "                \n",
    "                loss = outputs.loss\n",
    "                total_eval_loss += loss.item()\n",
    "                \n",
    "                predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "                eval_predictions.extend(predictions.cpu().numpy())\n",
    "                eval_true.extend(labels.cpu().numpy())\n",
    "        \n",
    "        avg_val_loss = total_eval_loss / len(val_dataloader)\n",
    "        val_accuracy = accuracy_score(eval_true, eval_predictions)\n",
    "        \n",
    "        print(f'üìà Validation Loss: {avg_val_loss:.4f} | Accuracy: {val_accuracy:.4f}')\n",
    "        \n",
    "        # Save best model\n",
    "        if val_accuracy > best_val_accuracy:\n",
    "            best_val_accuracy = val_accuracy\n",
    "            print(f'üèÜ New best validation accuracy: {best_val_accuracy:.4f}')\n",
    "        \n",
    "        # Print classification report for last epoch\n",
    "        if epoch == epochs - 1:\n",
    "            print(\"\\nüìã Final Classification Report:\")\n",
    "            print(classification_report(eval_true, eval_predictions, target_names=['Non-Hate', 'Hate']))\n",
    "    \n",
    "    return model\n",
    "\n",
    "def run_multilingual_hate_speech_training():\n",
    "    \"\"\"Main function to run multilingual hate speech detection training\"\"\"\n",
    "    \n",
    "    print(\"üöÄ Starting Multilingual Hate Speech Detection Training (Tamil + Hindi)\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # 1. Load and prepare combined data\n",
    "    df = load_and_prepare_combined_data()\n",
    "    \n",
    "    if df is None:\n",
    "        print(\"‚ùå Failed to load data. Exiting...\")\n",
    "        return\n",
    "    \n",
    "    # 2. Split the data (stratified by both label and language if possible)\n",
    "    print(f\"\\nüîÑ Splitting combined data...\")\n",
    "    train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "        df['text'].tolist(),\n",
    "        df['label'].tolist(),\n",
    "        test_size=0.2,\n",
    "        random_state=42,\n",
    "        stratify=df['label']  # Stratify by label\n",
    "    )\n",
    "    \n",
    "    print(f\"üìö Training samples: {len(train_texts)}\")\n",
    "    print(f\"üìù Validation samples: {len(val_texts)}\")\n",
    "    \n",
    "    # 3. Load model and tokenizer for multilingual support\n",
    "    # Using multilingual models that support both Tamil and Hindi\n",
    "    model_name = \"xlm-roberta-base\"  # Better for Tamil + Hindi\n",
    "    # Alternative: \"bert-base-multilingual-cased\" or \"distilbert-base-multilingual-cased\"\n",
    "    \n",
    "    print(f\"\\nü§ñ Loading multilingual model: {model_name}\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "    num_labels = len(set(df['label']))\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_name,\n",
    "        num_labels=num_labels,\n",
    "        ignore_mismatched_sizes=True\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ Model loaded with {num_labels} labels\")\n",
    "    \n",
    "    # 4. Create datasets and data loaders\n",
    "    print(f\"\\nüì¶ Creating datasets...\")\n",
    "    train_dataset = CustomDataset(train_texts, train_labels, tokenizer, max_length=128)\n",
    "    val_dataset = CustomDataset(val_texts, val_labels, tokenizer, max_length=128)\n",
    "    \n",
    "    batch_size = 16\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    print(f\"‚úÖ Data loaders created (batch size: {batch_size})\")\n",
    "    \n",
    "    # 5. Set device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"üíª Using device: {device}\")\n",
    "    \n",
    "    # 6. Train the model\n",
    "    print(f\"\\nüéØ Starting training...\")\n",
    "    trained_model = train_model(\n",
    "        model, \n",
    "        train_dataloader, \n",
    "        val_dataloader, \n",
    "        device,\n",
    "        epochs=5,  # Increased for multilingual training\n",
    "        lr=2e-5\n",
    "    )\n",
    "    \n",
    "    # 7. Save the model\n",
    "    output_dir = \"./trained_multilingual_hate_speech_model\"\n",
    "    print(f\"\\nüíæ Saving model to {output_dir}...\")\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    trained_model.save_pretrained(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "    \n",
    "    print(f\"\\nüéâ Training completed successfully!\")\n",
    "    print(f\"üìÅ Model saved to: {output_dir}\")\n",
    "    print(f\"üöÄ You can now use this model for multilingual hate speech detection!\")\n",
    "    \n",
    "    return trained_model, tokenizer\n",
    "\n",
    "def test_multilingual_predictions(model, tokenizer, device):\n",
    "    \"\"\"Test the trained model on sample texts in both languages\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    test_texts = [\n",
    "        # Tamil examples\n",
    "        \"‡Æá‡Æ§‡ØÅ ‡Æí‡Æ∞‡ØÅ ‡Æ®‡Æ≤‡Øç‡Æ≤ ‡Æö‡ØÜ‡ÆØ‡Øç‡Æ§‡Æø\",  # This is good news\n",
    "        \"‡ÆÖ‡Æ∞‡ØÅ‡ÆÆ‡Øà‡ÆØ‡Ææ‡Æ© ‡Æµ‡Øá‡Æ≤‡Øà\",  # Excellent work\n",
    "        \n",
    "        # Hindi examples  \n",
    "        \"‡§Ø‡§π ‡§¨‡§π‡•Å‡§§ ‡§Ö‡§ö‡•ç‡§õ‡§æ ‡§π‡•à\",  # This is very good\n",
    "        \"‡§¨‡§π‡•Å‡§§ ‡§¨‡§¢‡§º‡§ø‡§Ø‡§æ ‡§ï‡§æ‡§Æ\"  # Very good work\n",
    "    ]\n",
    "    \n",
    "    print(\"\\nüß™ Testing multilingual predictions:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for text in test_texts:\n",
    "        encoding = tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=128,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "         \n",
    "        with torch.no_grad():\n",
    "            input_ids = encoding['input_ids'].to(device)\n",
    "            attention_mask = encoding['attention_mask'].to(device)\n",
    "            \n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            prediction = torch.argmax(outputs.logits, dim=-1)\n",
    "            confidence = torch.softmax(outputs.logits, dim=-1)\n",
    "            \n",
    "        pred_label = \"Hate\" if prediction.item() == 1 else \"Non-Hate\"\n",
    "        conf_score = confidence.max().item()\n",
    "        \n",
    "        print(f\"Text: '{text}'\")\n",
    "        print(f\"Prediction: {pred_label} (Confidence: {conf_score:.4f})\")\n",
    "        print()\n",
    "\n",
    "# Run the training\n",
    "if __name__ == \"__main__\":\n",
    "    trained_model, tokenizer = run_multilingual_hate_speech_training()\n",
    "    \n",
    "    # Test the model if training was successful\n",
    "    if trained_model is not None:\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        test_multilingual_predictions(trained_model, tokenizer, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f1018e-cde1-4c34-a2eb-b3c897ccf406",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install hf_xet\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset as TorchDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "from datasets import Dataset\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set environment\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "print(\"‚úÖ All imports successful - no Trainer issues!\")\n",
    "\n",
    "class CustomDataset(TorchDataset):\n",
    "    \"\"\"Custom PyTorch Dataset for text classification\"\"\"\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "def preprocess_hindi_labels(label_str):\n",
    "    \"\"\"Convert Hindi multi-label format to binary classification\"\"\"\n",
    "    if pd.isna(label_str) or label_str == 'non-hostile':\n",
    "        return 0  # Non-hate\n",
    "    else:\n",
    "        return 1  # Hate (any form of hate/offensive/defamation)\n",
    "\n",
    "def load_and_prepare_combined_data():\n",
    "    \"\"\"Load and prepare the combined Tamil-Hindi datasets\"\"\"\n",
    "    try:\n",
    "        print(\"üìÇ Loading Tamil datasets...\")\n",
    "        # Tamil files\n",
    "        tamil_train = pd.read_csv(\"tamil_offensive_speech_train.csv\")[[\"comment\", \"label\"]]\n",
    "        tamil_val = pd.read_csv(\"tamil_offensive_speech_val.csv\")[[\"comment\", \"label\"]]\n",
    "        \n",
    "        # Rename 'comment' column to 'text' to be consistent with Hindi dataset\n",
    "        tamil_train = tamil_train.rename(columns={'comment': 'text'})\n",
    "        tamil_val = tamil_val.rename(columns={'comment': 'text'})\n",
    "        tamil_train[\"lang\"] = \"ta\"\n",
    "        tamil_val[\"lang\"] = \"ta\"\n",
    "        \n",
    "        print(f\"‚úÖ Tamil train: {len(tamil_train)} samples\")\n",
    "        print(f\"‚úÖ Tamil validation: {len(tamil_val)} samples\")\n",
    "        print(f\"üìä Tamil label distribution:\")\n",
    "        print(tamil_train['label'].value_counts())\n",
    "        \n",
    "        print(\"\\nüìÇ Loading Hindi datasets...\")\n",
    "        # Hindi files\n",
    "        hindi_train = pd.read_csv(\"Hatespeech-Hindi_Train.csv\")[[\"Post\", \"Labels Set\"]]\n",
    "        hindi_val = pd.read_csv(\"Hatespeech-Hindi_Valid.csv\")[[\"Post\", \"Labels Set\"]]\n",
    "        \n",
    "        # Rename columns to be consistent\n",
    "        hindi_train = hindi_train.rename(columns={'Post': 'text', 'Labels Set': 'label'})\n",
    "        hindi_val = hindi_val.rename(columns={'Post': 'text', 'Labels Set': 'label'})\n",
    "        \n",
    "        print(f\"‚úÖ Hindi train: {len(hindi_train)} samples\")\n",
    "        print(f\"‚úÖ Hindi validation: {len(hindi_val)} samples\")\n",
    "        print(f\"üìä Original Hindi label distribution:\")\n",
    "        print(hindi_train['label'].value_counts())\n",
    "        \n",
    "        # Process Hindi labels to binary format\n",
    "        hindi_train['label'] = hindi_train['label'].apply(preprocess_hindi_labels)\n",
    "        hindi_val['label'] = hindi_val['label'].apply(preprocess_hindi_labels)\n",
    "        \n",
    "        print(f\"üìä Processed Hindi label distribution:\")\n",
    "        print(hindi_train['label'].value_counts())\n",
    "        \n",
    "        hindi_train[\"lang\"] = \"hi\"\n",
    "        hindi_val[\"lang\"] = \"hi\"\n",
    "        \n",
    "        print(\"\\nüîÑ Combining datasets...\")\n",
    "        # Combine into df\n",
    "        df = pd.concat([tamil_train, tamil_val, hindi_train, hindi_val], ignore_index=True)\n",
    "        df.dropna(inplace=True)\n",
    "        \n",
    "        print(f\"‚úÖ Combined dataset: {len(df)} samples\")\n",
    "        print(f\"üìä Final label distribution:\")\n",
    "        print(df['label'].value_counts())\n",
    "        print(f\"üìä Language distribution:\")\n",
    "        print(df['lang'].value_counts())\n",
    "        \n",
    "        # Display sample data\n",
    "        print(f\"\\nüîç Sample data:\")\n",
    "        print(df.head())\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"‚ùå File not found: {e}\")\n",
    "        print(\"üìÅ Make sure all CSV files are in the same directory\")\n",
    "        return None\n",
    "\n",
    "def train_model(model, train_dataloader, val_dataloader, device, epochs=3, lr=2e-5):\n",
    "    \"\"\"Manual training loop - replaces Trainer\"\"\"\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    total_steps = len(train_dataloader) * epochs\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=0,\n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "    \n",
    "    model.to(device)\n",
    "    best_val_accuracy = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        print(f'\\nüìö Epoch {epoch + 1}/{epochs}')\n",
    "        print('-' * 50)\n",
    "        \n",
    "        # Training phase\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        train_predictions = []\n",
    "        train_true = []\n",
    "        \n",
    "        train_pbar = tqdm(train_dataloader, desc=\"Training\")\n",
    "        for batch in train_pbar:\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            outputs = model(input_ids=input_ids, \n",
    "                          attention_mask=attention_mask, \n",
    "                          labels=labels)\n",
    "            \n",
    "            loss = outputs.loss\n",
    "            total_train_loss += loss.item()\n",
    "            \n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "            train_predictions.extend(predictions.cpu().numpy())\n",
    "            train_true.extend(labels.cpu().numpy())\n",
    "            \n",
    "            train_pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "        \n",
    "        avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "        train_accuracy = accuracy_score(train_true, train_predictions)\n",
    "        \n",
    "        print(f'üìä Training Loss: {avg_train_loss:.4f} | Accuracy: {train_accuracy:.4f}')\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        total_eval_loss = 0\n",
    "        eval_predictions = []\n",
    "        eval_true = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            val_pbar = tqdm(val_dataloader, desc=\"Validation\")\n",
    "            for batch in val_pbar:\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "                \n",
    "                outputs = model(input_ids=input_ids, \n",
    "                              attention_mask=attention_mask, \n",
    "                              labels=labels)\n",
    "                \n",
    "                loss = outputs.loss\n",
    "                total_eval_loss += loss.item()\n",
    "                \n",
    "                predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "                eval_predictions.extend(predictions.cpu().numpy())\n",
    "                eval_true.extend(labels.cpu().numpy())\n",
    "        \n",
    "        avg_val_loss = total_eval_loss / len(val_dataloader)\n",
    "        val_accuracy = accuracy_score(eval_true, eval_predictions)\n",
    "        \n",
    "        print(f'üìà Validation Loss: {avg_val_loss:.4f} | Accuracy: {val_accuracy:.4f}')\n",
    "        \n",
    "        # Save best model\n",
    "        if val_accuracy > best_val_accuracy:\n",
    "            best_val_accuracy = val_accuracy\n",
    "            print(f'üèÜ New best validation accuracy: {best_val_accuracy:.4f}')\n",
    "        \n",
    "        # Print classification report for last epoch\n",
    "        if epoch == epochs - 1:\n",
    "            print(\"\\nüìã Final Classification Report:\")\n",
    "            print(classification_report(eval_true, eval_predictions, target_names=['Non-Hate', 'Hate']))\n",
    "    \n",
    "    return model\n",
    "\n",
    "def run_multilingual_hate_speech_training():\n",
    "    \"\"\"Main function to run multilingual hate speech detection training\"\"\"\n",
    "    \n",
    "    print(\"üöÄ Starting Multilingual Hate Speech Detection Training (Tamil + Hindi)\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # 1. Load and prepare combined data\n",
    "    df = load_and_prepare_combined_data()\n",
    "    \n",
    "    if df is None:\n",
    "        print(\"‚ùå Failed to load data. Exiting...\")\n",
    "        return\n",
    "    \n",
    "    # 2. Split the data (stratified by both label and language if possible)\n",
    "    print(f\"\\nüîÑ Splitting combined data...\")\n",
    "    train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "        df['text'].tolist(),\n",
    "        df['label'].tolist(),\n",
    "        test_size=0.2,\n",
    "        random_state=42,\n",
    "        stratify=df['label']  # Stratify by label\n",
    "    )\n",
    "    \n",
    "    print(f\"üìö Training samples: {len(train_texts)}\")\n",
    "    print(f\"üìù Validation samples: {len(val_texts)}\")\n",
    "    \n",
    "    # 3. Load model and tokenizer for multilingual support\n",
    "    # Using smaller multilingual model for faster CPU training\n",
    "    model_name = \"distilbert-base-multilingual-cased\"  # Much faster on CPU\n",
    "    # Alternative: \"bert-base-multilingual-cased\" (medium) or \"xlm-roberta-base\" (best but slowest)\n",
    "    \n",
    "    print(f\"\\nü§ñ Loading multilingual model: {model_name}\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "    num_labels = len(set(df['label']))\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_name,\n",
    "        num_labels=num_labels,\n",
    "        ignore_mismatched_sizes=True\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ Model loaded with {num_labels} labels\")\n",
    "    \n",
    "    # 4. Create datasets and data loaders\n",
    "    print(f\"\\nüì¶ Creating datasets...\")\n",
    "    train_dataset = CustomDataset(train_texts, train_labels, tokenizer, max_length=128)\n",
    "    val_dataset = CustomDataset(val_texts, val_labels, tokenizer, max_length=128)\n",
    "    \n",
    "    batch_size = 16\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    print(f\"‚úÖ Data loaders created (batch size: {batch_size})\")\n",
    "    \n",
    "    # 5. Set device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"üíª Using device: {device}\")\n",
    "    \n",
    "    # 6. Train the model\n",
    "    print(f\"\\nüéØ Starting training...\")\n",
    "    trained_model = train_model(\n",
    "        model, \n",
    "        train_dataloader, \n",
    "        val_dataloader, \n",
    "        device,\n",
    "        epochs=5,  # Increased for multilingual training\n",
    "        lr=2e-5\n",
    "    )\n",
    "    \n",
    "    # 7. Save the model\n",
    "    output_dir = \"./trained_multilingual_hate_speech_model\"\n",
    "    print(f\"\\nüíæ Saving model to {output_dir}...\")\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    trained_model.save_pretrained(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "    \n",
    "    print(f\"\\nüéâ Training completed successfully!\")\n",
    "    print(f\"üìÅ Model saved to: {output_dir}\")\n",
    "    print(f\"üöÄ You can now use this model for multilingual hate speech detection!\")\n",
    "    \n",
    "    return trained_model, tokenizer\n",
    "\n",
    "def test_multilingual_predictions(model, tokenizer, device):\n",
    "    \"\"\"Test the trained model on sample texts in both languages\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    test_texts = [\n",
    "        # Tamil examples\n",
    "        \"‡Æá‡Æ§‡ØÅ ‡Æí‡Æ∞‡ØÅ ‡Æ®‡Æ≤‡Øç‡Æ≤ ‡Æö‡ØÜ‡ÆØ‡Øç‡Æ§‡Æø\",  # This is good news\n",
    "        \"‡ÆÖ‡Æ∞‡ØÅ‡ÆÆ‡Øà‡ÆØ‡Ææ‡Æ© ‡Æµ‡Øá‡Æ≤‡Øà\",  # Excellent work\n",
    "        \n",
    "        # Hindi examples  \n",
    "        \"‡§Ø‡§π ‡§¨‡§π‡•Å‡§§ ‡§Ö‡§ö‡•ç‡§õ‡§æ ‡§π‡•à\",  # This is very good\n",
    "        \"‡§¨‡§π‡•Å‡§§ ‡§¨‡§¢‡§º‡§ø‡§Ø‡§æ ‡§ï‡§æ‡§Æ\"  # Very good work\n",
    "    ]\n",
    "    \n",
    "    print(\"\\nüß™ Testing multilingual predictions:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for text in test_texts:\n",
    "        encoding = tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=128,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            input_ids = encoding['input_ids'].to(device)\n",
    "            attention_mask = encoding['attention_mask'].to(device)\n",
    "            \n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            prediction = torch.argmax(outputs.logits, dim=-1)\n",
    "            confidence = torch.softmax(outputs.logits, dim=-1)\n",
    "            \n",
    "        pred_label = \"Hate\" if prediction.item() == 1 else \"Non-Hate\"\n",
    "        conf_score = confidence.max().item()\n",
    "        \n",
    "        print(f\"Text: '{text}'\")\n",
    "        print(f\"Prediction: {pred_label} (Confidence: {conf_score:.4f})\")\n",
    "        print()\n",
    "\n",
    "# Run the training\n",
    "if __name__ == \"__main__\":\n",
    "    trained_model, tokenizer = run_multilingual_hate_speech_training()\n",
    "    \n",
    "    # Test the model if training was successful\n",
    "    if trained_model is not None:\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        test_multilingual_predictions(trained_model, tokenizer, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "452b8384-5f04-4b64-9f7d-1d1a7efa4cb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting hf_xet\n",
      "  Downloading hf_xet-1.1.2-cp37-abi3-win_amd64.whl.metadata (883 bytes)\n",
      "Downloading hf_xet-1.1.2-cp37-abi3-win_amd64.whl (2.7 MB)\n",
      "   ---------------------------------------- 0.0/2.7 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 0.3/2.7 MB ? eta -:--:--\n",
      "   ------------------- -------------------- 1.3/2.7 MB 3.7 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 2.1/2.7 MB 4.3 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 2.6/2.7 MB 3.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.7/2.7 MB 3.4 MB/s eta 0:00:00\n",
      "Installing collected packages: hf_xet\n",
      "Successfully installed hf_xet-1.1.2\n",
      "‚úÖ All imports successful - no Trainer issues!\n",
      "\n",
      "üîç Checking hardware...\n",
      "CUDA available: False\n",
      "Device count: 0\n",
      "‚ö†Ô∏è  Using CPU - training will be slower\n",
      "üåü Multilingual Hate Speech Detection Training\n",
      "üîß Optimized for CPU with sample data for fast testing\n",
      "================================================================================\n",
      "üöÄ Starting Multilingual Hate Speech Detection Training (Tamil + Hindi)\n",
      "================================================================================\n",
      "üìÇ Loading Tamil datasets...\n",
      "‚úÖ Tamil train: 27875 samples\n",
      "‚úÖ Tamil validation: 6969 samples\n",
      "üìä Tamil label distribution:\n",
      "label\n",
      "0    21226\n",
      "1     6649\n",
      "Name: count, dtype: int64\n",
      "\n",
      "üìÇ Loading Hindi datasets...\n",
      "‚úÖ Hindi train: 5728 samples\n",
      "‚úÖ Hindi validation: 811 samples\n",
      "üìä Original Hindi label distribution:\n",
      "label\n",
      "non-hostile                       3050\n",
      "fake                              1009\n",
      "hate                               478\n",
      "offensive                          405\n",
      "defamation                         305\n",
      "hate,offensive                     163\n",
      "defamation,offensive                81\n",
      "defamation,hate                     74\n",
      "defamation,fake                     34\n",
      "defamation,hate,offensive           28\n",
      "fake,offensive                      28\n",
      "fake,hate                           27\n",
      "defamation,fake,offensive           24\n",
      "defamation,fake,hate                 9\n",
      "defamation,fake,hate,offensive       9\n",
      "fake,hate,offensive                  4\n",
      "Name: count, dtype: int64\n",
      "üìä Processed Hindi label distribution:\n",
      "label\n",
      "0    3050\n",
      "1    2678\n",
      "Name: count, dtype: int64\n",
      "\n",
      "üîÑ Combining datasets...\n",
      "‚úÖ Combined dataset: 41378 samples\n",
      "üìä Final label distribution:\n",
      "label\n",
      "0    29993\n",
      "1    11385\n",
      "Name: count, dtype: int64\n",
      "üìä Language distribution:\n",
      "lang\n",
      "ta    34839\n",
      "hi     6539\n",
      "Name: count, dtype: int64\n",
      "\n",
      "üîç Sample data:\n",
      "                                                text  label lang\n",
      "0                  omg that bgm make me goosebumb...      0   ta\n",
      "1         neraya neraya neraya neraya neraya neraya.      0   ta\n",
      "2  thalaivar mersal look .semma massss thalaiva ....      0   ta\n",
      "3  paaaa... repeat mode.... adra adra adraaaaa......      0   ta\n",
      "4  epaa ena panaporam... sweet sapade poram... aw...      0   ta\n",
      "\n",
      "üß™ Using sample for faster testing...\n",
      "Sample size: 5000 samples\n",
      "Sample label distribution:\n",
      "label\n",
      "0    3596\n",
      "1    1404\n",
      "Name: count, dtype: int64\n",
      "Sample language distribution:\n",
      "lang\n",
      "ta    4194\n",
      "hi     806\n",
      "Name: count, dtype: int64\n",
      "\n",
      "üîÑ Splitting combined data...\n",
      "üìö Training samples: 4000\n",
      "üìù Validation samples: 1000\n",
      "\n",
      "üìä Training Configuration:\n",
      "   ü§ñ Model: distilbert-base-multilingual-cased\n",
      "   üì¶ Batch size: 8\n",
      "   üîÑ Epochs: 2\n",
      "   üìè Max length: 64\n",
      "   üìà Learning rate: 2e-05\n",
      "\n",
      "ü§ñ Loading multilingual model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model loaded with 2 labels\n",
      "\n",
      "üì¶ Creating datasets...\n",
      "‚úÖ Data loaders created\n",
      "   üìö Training batches: 500\n",
      "   üìù Validation batches: 125\n",
      "\n",
      "üíª Using device: cpu\n",
      "‚è±Ô∏è  Estimated training time: ~50 minutes\n",
      "\n",
      "üéØ Starting training...\n",
      "\n",
      "üìö Epoch 1/2\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [24:27<00:00,  2.93s/it, loss=0.9763]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Training Loss: 0.4894 | Accuracy: 0.7705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 125/125 [01:01<00:00,  2.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìà Validation Loss: 0.4220 | Accuracy: 0.8080\n",
      "üèÜ New best validation accuracy: 0.8080\n",
      "\n",
      "üìö Epoch 2/2\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [24:27<00:00,  2.94s/it, loss=0.1580]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Training Loss: 0.3708 | Accuracy: 0.8277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 125/125 [01:02<00:00,  2.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìà Validation Loss: 0.4228 | Accuracy: 0.8190\n",
      "üèÜ New best validation accuracy: 0.8190\n",
      "\n",
      "üìã Final Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Non-Hate       0.84      0.92      0.88       719\n",
      "        Hate       0.73      0.57      0.64       281\n",
      "\n",
      "    accuracy                           0.82      1000\n",
      "   macro avg       0.79      0.74      0.76      1000\n",
      "weighted avg       0.81      0.82      0.81      1000\n",
      "\n",
      "\n",
      "üíæ Saving model to ./trained_multilingual_hate_speech_model...\n",
      "\n",
      "üéâ Training completed successfully!\n",
      "üìÅ Model saved to: ./trained_multilingual_hate_speech_model\n",
      "üöÄ You can now use this model for multilingual hate speech detection!\n",
      "\n",
      "üß™ Testing multilingual predictions:\n",
      "------------------------------------------------------------\n",
      "1. Text: '‡Æá‡Æ§‡ØÅ ‡Æí‡Æ∞‡ØÅ ‡Æ®‡Æ≤‡Øç‡Æ≤ ‡Æö‡ØÜ‡ÆØ‡Øç‡Æ§‡Æø'\n",
      "   Prediction: Non-Hate (Confidence: 0.9728)\n",
      "\n",
      "2. Text: '‡ÆÖ‡Æ∞‡ØÅ‡ÆÆ‡Øà‡ÆØ‡Ææ‡Æ© ‡Æµ‡Øá‡Æ≤‡Øà'\n",
      "   Prediction: Non-Hate (Confidence: 0.9596)\n",
      "\n",
      "3. Text: '‡Æµ‡Ææ‡Æ¥‡Øç‡Æ§‡Øç‡Æ§‡ØÅ‡Æï‡Øç‡Æï‡Æ≥‡Øç'\n",
      "   Prediction: Non-Hate (Confidence: 0.9866)\n",
      "\n",
      "4. Text: '‡§Ø‡§π ‡§¨‡§π‡•Å‡§§ ‡§Ö‡§ö‡•ç‡§õ‡§æ ‡§π‡•à'\n",
      "   Prediction: Hate (Confidence: 0.9501)\n",
      "\n",
      "5. Text: '‡§¨‡§π‡•Å‡§§ ‡§¨‡§¢‡§º‡§ø‡§Ø‡§æ ‡§ï‡§æ‡§Æ'\n",
      "   Prediction: Hate (Confidence: 0.8974)\n",
      "\n",
      "6. Text: '‡§ß‡§®‡•ç‡§Ø‡§µ‡§æ‡§¶'\n",
      "   Prediction: Non-Hate (Confidence: 0.6652)\n",
      "\n",
      "7. Text: 'Great job! ‡§¨‡§π‡•Å‡§§ ‡§Ö‡§ö‡•ç‡§õ‡§æ'\n",
      "   Prediction: Hate (Confidence: 0.6365)\n",
      "\n",
      "8. Text: '‡Æµ‡Ææ‡Æ¥‡Øç‡Æ§‡Øç‡Æ§‡ØÅ‡Æï‡Øç‡Æï‡Æ≥‡Øç friend!'\n",
      "   Prediction: Non-Hate (Confidence: 0.9833)\n",
      "\n",
      "\n",
      "================================================================================\n",
      "üéØ Training Summary:\n",
      "‚úÖ Model: DistilBERT Multilingual (optimized for speed)\n",
      "‚úÖ Languages: Tamil + Hindi\n",
      "‚úÖ Task: Binary hate speech classification\n",
      "‚úÖ Optimization: Sample data + reduced parameters for CPU\n",
      "üí° To use full dataset: Set USE_SAMPLE = False in load_and_prepare_combined_data()\n",
      "üí° For production: Use GPU and increase epochs to 5+\n"
     ]
    }
   ],
   "source": [
    "!pip install hf_xet\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset as TorchDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "from datasets import Dataset\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set environment\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "print(\"‚úÖ All imports successful - no Trainer issues!\")\n",
    "\n",
    "# Hardware check\n",
    "print(\"\\nüîç Checking hardware...\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"Device count: {torch.cuda.device_count()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name()}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Using CPU - training will be slower\")\n",
    "\n",
    "class CustomDataset(TorchDataset):\n",
    "    \"\"\"Custom PyTorch Dataset for text classification\"\"\"\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=64):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "def preprocess_hindi_labels(label_str):\n",
    "    \"\"\"Convert Hindi multi-label format to binary classification\"\"\"\n",
    "    if pd.isna(label_str) or label_str == 'non-hostile':\n",
    "        return 0  # Non-hate\n",
    "    else:\n",
    "        return 1  # Hate (any form of hate/offensive/defamation)\n",
    "\n",
    "def load_and_prepare_combined_data():\n",
    "    \"\"\"Load and prepare the combined Tamil-Hindi datasets\"\"\"\n",
    "    try:\n",
    "        print(\"üìÇ Loading Tamil datasets...\")\n",
    "        # Tamil files\n",
    "        tamil_train = pd.read_csv(\"tamil_offensive_speech_train.csv\")[[\"comment\", \"label\"]]\n",
    "        tamil_val = pd.read_csv(\"tamil_offensive_speech_val.csv\")[[\"comment\", \"label\"]]\n",
    "        \n",
    "        # Rename 'comment' column to 'text' to be consistent with Hindi dataset\n",
    "        tamil_train = tamil_train.rename(columns={'comment': 'text'})\n",
    "        tamil_val = tamil_val.rename(columns={'comment': 'text'})\n",
    "        tamil_train[\"lang\"] = \"ta\"\n",
    "        tamil_val[\"lang\"] = \"ta\"\n",
    "        \n",
    "        print(f\"‚úÖ Tamil train: {len(tamil_train)} samples\")\n",
    "        print(f\"‚úÖ Tamil validation: {len(tamil_val)} samples\")\n",
    "        print(f\"üìä Tamil label distribution:\")\n",
    "        print(tamil_train['label'].value_counts())\n",
    "        \n",
    "        print(\"\\nüìÇ Loading Hindi datasets...\")\n",
    "        # Hindi files\n",
    "        hindi_train = pd.read_csv(\"Hatespeech-Hindi_Train.csv\")[[\"Post\", \"Labels Set\"]]\n",
    "        hindi_val = pd.read_csv(\"Hatespeech-Hindi_Valid.csv\")[[\"Post\", \"Labels Set\"]]\n",
    "        \n",
    "        # Rename columns to be consistent\n",
    "        hindi_train = hindi_train.rename(columns={'Post': 'text', 'Labels Set': 'label'})\n",
    "        hindi_val = hindi_val.rename(columns={'Post': 'text', 'Labels Set': 'label'})\n",
    "        \n",
    "        print(f\"‚úÖ Hindi train: {len(hindi_train)} samples\")\n",
    "        print(f\"‚úÖ Hindi validation: {len(hindi_val)} samples\")\n",
    "        print(f\"üìä Original Hindi label distribution:\")\n",
    "        print(hindi_train['label'].value_counts())\n",
    "        \n",
    "        # Process Hindi labels to binary format\n",
    "        hindi_train['label'] = hindi_train['label'].apply(preprocess_hindi_labels)\n",
    "        hindi_val['label'] = hindi_val['label'].apply(preprocess_hindi_labels)\n",
    "        \n",
    "        print(f\"üìä Processed Hindi label distribution:\")\n",
    "        print(hindi_train['label'].value_counts())\n",
    "        \n",
    "        hindi_train[\"lang\"] = \"hi\"\n",
    "        hindi_val[\"lang\"] = \"hi\"\n",
    "        \n",
    "        print(\"\\nüîÑ Combining datasets...\")\n",
    "        # Combine into df\n",
    "        df = pd.concat([tamil_train, tamil_val, hindi_train, hindi_val], ignore_index=True)\n",
    "        df.dropna(inplace=True)\n",
    "        \n",
    "        print(f\"‚úÖ Combined dataset: {len(df)} samples\")\n",
    "        print(f\"üìä Final label distribution:\")\n",
    "        print(df['label'].value_counts())\n",
    "        print(f\"üìä Language distribution:\")\n",
    "        print(df['lang'].value_counts())\n",
    "        \n",
    "        # Display sample data\n",
    "        print(f\"\\nüîç Sample data:\")\n",
    "        print(df.head())\n",
    "        \n",
    "        # OPTIMIZATION: Use sample for faster testing\n",
    "        USE_SAMPLE = True  # Set to False for full training\n",
    "        \n",
    "        if USE_SAMPLE:\n",
    "            print(f\"\\nüß™ Using sample for faster testing...\")\n",
    "            df_sample = df.sample(n=5000, random_state=42)\n",
    "            print(f\"Sample size: {len(df_sample)} samples\")\n",
    "            print(f\"Sample label distribution:\")\n",
    "            print(df_sample['label'].value_counts())\n",
    "            print(f\"Sample language distribution:\")\n",
    "            print(df_sample['lang'].value_counts())\n",
    "            return df_sample\n",
    "        else:\n",
    "            print(f\"\\nüî• Using full dataset for training...\")\n",
    "            return df\n",
    "        \n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"‚ùå File not found: {e}\")\n",
    "        print(\"üìÅ Make sure all CSV files are in the same directory\")\n",
    "        return None\n",
    "\n",
    "def train_model(model, train_dataloader, val_dataloader, device, epochs=2, lr=2e-5):\n",
    "    \"\"\"Manual training loop - replaces Trainer\"\"\"\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    total_steps = len(train_dataloader) * epochs\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=0,\n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "    \n",
    "    model.to(device)\n",
    "    best_val_accuracy = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        print(f'\\nüìö Epoch {epoch + 1}/{epochs}')\n",
    "        print('-' * 50)\n",
    "        \n",
    "        # Training phase\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        train_predictions = []\n",
    "        train_true = []\n",
    "        \n",
    "        train_pbar = tqdm(train_dataloader, desc=\"Training\")\n",
    "        for batch in train_pbar:\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            outputs = model(input_ids=input_ids, \n",
    "                          attention_mask=attention_mask, \n",
    "                          labels=labels)\n",
    "            \n",
    "            loss = outputs.loss\n",
    "            total_train_loss += loss.item()\n",
    "            \n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "            train_predictions.extend(predictions.cpu().numpy())\n",
    "            train_true.extend(labels.cpu().numpy())\n",
    "            \n",
    "            train_pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "        \n",
    "        avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "        train_accuracy = accuracy_score(train_true, train_predictions)\n",
    "        \n",
    "        print(f'üìä Training Loss: {avg_train_loss:.4f} | Accuracy: {train_accuracy:.4f}')\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        total_eval_loss = 0\n",
    "        eval_predictions = []\n",
    "        eval_true = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            val_pbar = tqdm(val_dataloader, desc=\"Validation\")\n",
    "            for batch in val_pbar:\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "                \n",
    "                outputs = model(input_ids=input_ids, \n",
    "                              attention_mask=attention_mask, \n",
    "                              labels=labels)\n",
    "                \n",
    "                loss = outputs.loss\n",
    "                total_eval_loss += loss.item()\n",
    "                \n",
    "                predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "                eval_predictions.extend(predictions.cpu().numpy())\n",
    "                eval_true.extend(labels.cpu().numpy())\n",
    "        \n",
    "        avg_val_loss = total_eval_loss / len(val_dataloader)\n",
    "        val_accuracy = accuracy_score(eval_true, eval_predictions)\n",
    "        \n",
    "        print(f'üìà Validation Loss: {avg_val_loss:.4f} | Accuracy: {val_accuracy:.4f}')\n",
    "        \n",
    "        # Save best model\n",
    "        if val_accuracy > best_val_accuracy:\n",
    "            best_val_accuracy = val_accuracy\n",
    "            print(f'üèÜ New best validation accuracy: {best_val_accuracy:.4f}')\n",
    "        \n",
    "        # Print classification report for last epoch\n",
    "        if epoch == epochs - 1:\n",
    "            print(\"\\nüìã Final Classification Report:\")\n",
    "            print(classification_report(eval_true, eval_predictions, target_names=['Non-Hate', 'Hate']))\n",
    "    \n",
    "    return model\n",
    "\n",
    "def run_multilingual_hate_speech_training():\n",
    "    \"\"\"Main function to run multilingual hate speech detection training\"\"\"\n",
    "    \n",
    "    print(\"üöÄ Starting Multilingual Hate Speech Detection Training (Tamil + Hindi)\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # 1. Load and prepare combined data\n",
    "    df = load_and_prepare_combined_data()\n",
    "    \n",
    "    if df is None:\n",
    "        print(\"‚ùå Failed to load data. Exiting...\")\n",
    "        return\n",
    "    \n",
    "    # 2. Split the data (stratified by both label and language if possible)\n",
    "    print(f\"\\nüîÑ Splitting combined data...\")\n",
    "    train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "        df['text'].tolist(),\n",
    "        df['label'].tolist(),\n",
    "        test_size=0.2,\n",
    "        random_state=42,\n",
    "        stratify=df['label']  # Stratify by label\n",
    "    )\n",
    "    \n",
    "    print(f\"üìö Training samples: {len(train_texts)}\")\n",
    "    print(f\"üìù Validation samples: {len(val_texts)}\")\n",
    "    \n",
    "    # 3. Optimized model and parameters\n",
    "    model_name = \"distilbert-base-multilingual-cased\"  # Fast multilingual model\n",
    "    batch_size = 8      # Reduced for CPU efficiency\n",
    "    epochs = 2          # Reduced for testing\n",
    "    max_length = 64     # Reduced for speed\n",
    "    learning_rate = 2e-5\n",
    "    \n",
    "    print(f\"\\nüìä Training Configuration:\")\n",
    "    print(f\"   ü§ñ Model: {model_name}\")\n",
    "    print(f\"   üì¶ Batch size: {batch_size}\")\n",
    "    print(f\"   üîÑ Epochs: {epochs}\")\n",
    "    print(f\"   üìè Max length: {max_length}\")\n",
    "    print(f\"   üìà Learning rate: {learning_rate}\")\n",
    "    \n",
    "    # 4. Load model and tokenizer\n",
    "    print(f\"\\nü§ñ Loading multilingual model...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "    num_labels = len(set(df['label']))\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_name,\n",
    "        num_labels=num_labels,\n",
    "        ignore_mismatched_sizes=True\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ Model loaded with {num_labels} labels\")\n",
    "    \n",
    "    # 5. Create datasets and data loaders\n",
    "    print(f\"\\nüì¶ Creating datasets...\")\n",
    "    train_dataset = CustomDataset(train_texts, train_labels, tokenizer, max_length=max_length)\n",
    "    val_dataset = CustomDataset(val_texts, val_labels, tokenizer, max_length=max_length)\n",
    "    \n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    print(f\"‚úÖ Data loaders created\")\n",
    "    print(f\"   üìö Training batches: {len(train_dataloader)}\")\n",
    "    print(f\"   üìù Validation batches: {len(val_dataloader)}\")\n",
    "    \n",
    "    # 6. Set device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"\\nüíª Using device: {device}\")\n",
    "    \n",
    "    # Estimate training time\n",
    "    if device.type == 'cpu':\n",
    "        estimated_time = len(train_dataloader) * epochs * 3  # ~3 seconds per batch on CPU\n",
    "        print(f\"‚è±Ô∏è  Estimated training time: ~{estimated_time//60} minutes\")\n",
    "    \n",
    "    # 7. Train the model\n",
    "    print(f\"\\nüéØ Starting training...\")\n",
    "    trained_model = train_model(\n",
    "        model, \n",
    "        train_dataloader, \n",
    "        val_dataloader, \n",
    "        device,\n",
    "        epochs=epochs,\n",
    "        lr=learning_rate\n",
    "    )\n",
    "    \n",
    "    # 8. Save the model\n",
    "    output_dir = \"./trained_multilingual_hate_speech_model\"\n",
    "    print(f\"\\nüíæ Saving model to {output_dir}...\")\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    trained_model.save_pretrained(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "    \n",
    "    print(f\"\\nüéâ Training completed successfully!\")\n",
    "    print(f\"üìÅ Model saved to: {output_dir}\")\n",
    "    print(f\"üöÄ You can now use this model for multilingual hate speech detection!\")\n",
    "    \n",
    "    return trained_model, tokenizer\n",
    "\n",
    "def test_multilingual_predictions(model, tokenizer, device):\n",
    "    \"\"\"Test the trained model on sample texts in both languages\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    test_texts = [\n",
    "        # Tamil examples (safe)\n",
    "        \"‡Æá‡Æ§‡ØÅ ‡Æí‡Æ∞‡ØÅ ‡Æ®‡Æ≤‡Øç‡Æ≤ ‡Æö‡ØÜ‡ÆØ‡Øç‡Æ§‡Æø\",  # This is good news\n",
    "        \"‡ÆÖ‡Æ∞‡ØÅ‡ÆÆ‡Øà‡ÆØ‡Ææ‡Æ© ‡Æµ‡Øá‡Æ≤‡Øà\",      # Excellent work\n",
    "        \"‡Æµ‡Ææ‡Æ¥‡Øç‡Æ§‡Øç‡Æ§‡ØÅ‡Æï‡Øç‡Æï‡Æ≥‡Øç\",        # Congratulations\n",
    "        \n",
    "        # Hindi examples (safe)\n",
    "        \"‡§Ø‡§π ‡§¨‡§π‡•Å‡§§ ‡§Ö‡§ö‡•ç‡§õ‡§æ ‡§π‡•à\",      # This is very good\n",
    "        \"‡§¨‡§π‡•Å‡§§ ‡§¨‡§¢‡§º‡§ø‡§Ø‡§æ ‡§ï‡§æ‡§Æ\",       # Very good work\n",
    "        \"‡§ß‡§®‡•ç‡§Ø‡§µ‡§æ‡§¶\",              # Thank you\n",
    "        \n",
    "        # Mixed content for testing\n",
    "        \"Great job! ‡§¨‡§π‡•Å‡§§ ‡§Ö‡§ö‡•ç‡§õ‡§æ\",  # Mixed language\n",
    "        \"‡Æµ‡Ææ‡Æ¥‡Øç‡Æ§‡Øç‡Æ§‡ØÅ‡Æï‡Øç‡Æï‡Æ≥‡Øç friend!\"   # Mixed language\n",
    "    ]\n",
    "    \n",
    "    print(\"\\nüß™ Testing multilingual predictions:\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for i, text in enumerate(test_texts, 1):\n",
    "        encoding = tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=64,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            input_ids = encoding['input_ids'].to(device)\n",
    "            attention_mask = encoding['attention_mask'].to(device)\n",
    "            \n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            prediction = torch.argmax(outputs.logits, dim=-1)\n",
    "            confidence = torch.softmax(outputs.logits, dim=-1)\n",
    "            \n",
    "        pred_label = \"Hate\" if prediction.item() == 1 else \"Non-Hate\"\n",
    "        conf_score = confidence.max().item()\n",
    "        \n",
    "        print(f\"{i}. Text: '{text}'\")\n",
    "        print(f\"   Prediction: {pred_label} (Confidence: {conf_score:.4f})\")\n",
    "        print()\n",
    "\n",
    "# Run the training\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"üåü Multilingual Hate Speech Detection Training\")\n",
    "    print(\"üîß Optimized for CPU with sample data for fast testing\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    trained_model, tokenizer = run_multilingual_hate_speech_training()\n",
    "    \n",
    "    # Test the model if training was successful\n",
    "    if trained_model is not None:\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        test_multilingual_predictions(trained_model, tokenizer, device)\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"üéØ Training Summary:\")\n",
    "        print(\"‚úÖ Model: DistilBERT Multilingual (optimized for speed)\")\n",
    "        print(\"‚úÖ Languages: Tamil + Hindi\")\n",
    "        print(\"‚úÖ Task: Binary hate speech classification\")\n",
    "        print(\"‚úÖ Optimization: Sample data + reduced parameters for CPU\")\n",
    "        print(\"üí° To use full dataset: Set USE_SAMPLE = False in load_and_prepare_combined_data()\")\n",
    "        print(\"üí° For production: Use GPU and increase epochs to 5+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c973a03-8a98-4acc-94fb-e9f11678482f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Dataset loaded successfully!\n",
      "Raw dataset shape: (27875, 2)\n",
      "Columns: ['label', 'comment']\n",
      "Dataset shape: (27875, 2)\n",
      "Label distribution:\n",
      "label\n",
      "0    21226\n",
      "1     6649\n",
      "Name: count, dtype: int64\n",
      "‚úÖ Data preprocessing completed!\n"
     ]
    }
   ],
   "source": [
    "# ‚úÖ Step 1: Load your dataset (choose one)\n",
    "# For Hindi dataset\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"Hatespeech-Hindi_Train.csv\")\n",
    "# For Tamil dataset\n",
    "df = pd.read_csv(\"tamil_offensive_speech_train.csv\")\n",
    "\n",
    "print(\"‚úÖ Dataset loaded successfully!\")\n",
    "print(f\"Raw dataset shape: {df.shape}\")\n",
    "print(f\"Columns: {list(df.columns)}\")\n",
    "# ‚úÖ Step 2: Rename columns to standardize\n",
    "df = df.rename(columns={\n",
    "    'comment': 'text',    # Replace 'comment' with your actual column name\n",
    "    'category': 'label'   # Replace 'category' with your actual label column\n",
    "})\n",
    "\n",
    "# ‚úÖ Step 3: Clean and prepare data\n",
    "df = df[['text', 'label']]  # Keep only relevant columns\n",
    "df['text'] = df['text'].astype(str)\n",
    "df['label'] = df['label'].astype(int)\n",
    "\n",
    "# Remove any rows with missing values\n",
    "df = df.dropna()\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Label distribution:\\n{df['label'].value_counts()}\")\n",
    "print(\"‚úÖ Data preprocessing completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f469cee8-20bc-4e86-bdba-af968a441d84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 22300\n",
      "Validation set size: 5575\n",
      "‚úÖ Train-validation split completed!\n"
     ]
    }
   ],
   "source": [
    "# ‚úÖ Step 4: Split into train/validation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset\n",
    "train_df, val_df = train_test_split(\n",
    "    df, \n",
    "    test_size=0.2, \n",
    "    stratify=df['label'], \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {len(train_df)}\")\n",
    "print(f\"Validation set size: {len(val_df)}\")\n",
    "\n",
    "# Convert to HuggingFace datasets\n",
    "train_dataset = Dataset.from_pandas(train_df.reset_index(drop=True))\n",
    "val_dataset = Dataset.from_pandas(val_df.reset_index(drop=True))\n",
    "\n",
    "print(\"‚úÖ Train-validation split completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "db813238-ef25-4283-894e-514bef990402",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer: xlm-roberta-base\n",
      "‚úÖ Tokenizer loaded successfully!\n",
      "Tokenizer vocab size: 250002\n"
     ]
    }
   ],
   "source": [
    "# ‚úÖ Step 5: Initialize tokenizer\n",
    "from transformers import AutoTokenizer\n",
    "model_name = \"xlm-roberta-base\"\n",
    "print(f\"Loading tokenizer: {model_name}\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "print(\"‚úÖ Tokenizer loaded successfully!\")\n",
    "print(f\"Tokenizer vocab size: {tokenizer.vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f0a1aa92-1e99-4f27-9de3-2fefec2b827c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Tokenization function defined!\n"
     ]
    }
   ],
   "source": [
    "# ‚úÖ Step 6: Tokenization function\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples['text'], \n",
    "        truncation=True, \n",
    "        padding=False,  # Padding will be handled by data collator\n",
    "        max_length=512\n",
    "    )\n",
    "\n",
    "print(\"‚úÖ Tokenization function defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "beaed8af-50d0-4a92-8a38-be9ba531e4ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing training dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3e260be74564c87921d9fae2400727a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/22300 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Training dataset tokenized!\n",
      "Training dataset features: {'text': Value(dtype='string', id=None), 'label': Value(dtype='int32', id=None), 'input_ids': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None), 'attention_mask': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None)}\n"
     ]
    }
   ],
   "source": [
    "# Apply tokenization to training dataset\n",
    "print(\"Tokenizing training dataset...\")\n",
    "train_dataset = Dataset.from_pandas(train_df.reset_index(drop=True))\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "print(\"‚úÖ Training dataset tokenized!\")\n",
    "print(f\"Training dataset features: {train_dataset.features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "28cfde3b-99dd-4050-89a2-a0f8a28fcb0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing validation dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79d5b0292a524bb685f8098239faa543",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5575 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Validation dataset tokenized!\n",
      "Validation dataset features: {'text': Value(dtype='string', id=None), 'label': Value(dtype='int32', id=None), 'input_ids': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None), 'attention_mask': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None)}\n"
     ]
    }
   ],
   "source": [
    "# Apply tokenization to validation dataset\n",
    "val_dataset = Dataset.from_pandas(val_df.reset_index(drop=True))\n",
    "print(\"Tokenizing validation dataset...\")\n",
    "val_dataset = val_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "print(\"‚úÖ Validation dataset tokenized!\")\n",
    "print(f\"Validation dataset features: {val_dataset.features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2fc37762-9c79-4d14-b298-def2fb832152",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of labels: 2\n",
      "Loading model: xlm-roberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model loaded successfully!\n",
      "Model device: cpu\n"
     ]
    }
   ],
   "source": [
    "# ‚úÖ Step 7: Initialize model\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "num_labels = len(df['label'].unique())\n",
    "print(f\"Number of labels: {num_labels}\")\n",
    "\n",
    "print(f\"Loading model: {model_name}\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=num_labels,\n",
    "    problem_type=\"single_label_classification\"\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Model loaded successfully!\")\n",
    "print(f\"Model device: {next(model.parameters()).device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b9ab610f-128c-45ba-a8f7-5caed792c081",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Final Setup Check:\n",
      "‚úÖ Dataset shape: (27875, 2)\n",
      "‚úÖ Number of labels: 2\n",
      "‚úÖ Training samples: 22300\n",
      "‚úÖ Validation samples: 5575\n",
      "‚úÖ Tokenizer loaded: True\n",
      "‚úÖ Model loaded: True\n",
      "‚úÖ Sample tokenization works: 33 tokens\n",
      "\n",
      "üéâ All setup complete! Ready for training configuration.\n"
     ]
    }
   ],
   "source": [
    "# Final check - make sure everything is loaded properly\n",
    "print(\"üîç Final Setup Check:\")\n",
    "print(f\"‚úÖ Dataset shape: {df.shape}\")\n",
    "print(f\"‚úÖ Number of labels: {num_labels}\")\n",
    "print(f\"‚úÖ Training samples: {len(train_dataset)}\")\n",
    "print(f\"‚úÖ Validation samples: {len(val_dataset)}\")\n",
    "print(f\"‚úÖ Tokenizer loaded: {tokenizer is not None}\")\n",
    "print(f\"‚úÖ Model loaded: {model is not None}\")\n",
    "\n",
    "# Test tokenization on a sample\n",
    "sample_text = train_df.iloc[0]['text']\n",
    "sample_tokens = tokenizer(sample_text, truncation=True, max_length=512)\n",
    "print(f\"‚úÖ Sample tokenization works: {len(sample_tokens['input_ids'])} tokens\")\n",
    "\n",
    "print(\"\\nüéâ All setup complete! Ready for training configuration.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0f449b54-1ed4-4af8-be32-9aaaade6d11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    accuracy = (predictions == labels).mean()\n",
    "    return {\"accuracy\": accuracy}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "64dc24c6-d486-4412-9bbc-aa3d08d2cf95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Data collator created successfully!\n"
     ]
    }
   ],
   "source": [
    "# Add this import\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "# Create data collator\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "print(\"‚úÖ Data collator created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "51220b64-2e19-4855-a0f7-9520d0f3661e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done succesfully\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "import torch\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"epoch\",        # Evaluate every epoch\n",
    "    save_strategy=\"epoch\",        # Save every epoch\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,  # Reduced from 16 to avoid memory issues\n",
    "    per_device_eval_batch_size=8,   # Reduced from 16 to avoid memory issues\n",
    "    num_train_epochs=4,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=100,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_accuracy\",\n",
    "    greater_is_better=True,\n",
    "    save_total_limit=2,\n",
    "    seed=42,\n",
    "    dataloader_pin_memory=False,\n",
    "    report_to=None,\n",
    "    fp16=True if torch.cuda.is_available() else False  # Enable mixed precision if GPU available\n",
    ")\n",
    "print(\"Done succesfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bd78859a-dd40-46d8-ad00-072428be2e6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking training setup...\n",
      "Model device: cpu\n",
      "Available GPU: False\n"
     ]
    }
   ],
   "source": [
    "# Add debugging information\n",
    "print(\"Checking training setup...\")\n",
    "# Check if CUDA is available and move model to GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Example with dummy data - replace with your actual data\n",
    "# Assuming you have features (X) and labels (y)\n",
    "X = torch.randn(1000, 10)  # 1000 samples, 10 features\n",
    "y = torch.randint(0, 2, (1000,))  # Binary classification\n",
    "\n",
    "# Create dataset and dataloader\n",
    "dataset = TensorDataset(X, y)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "# Also move your data to GPU during training\n",
    "for batch in dataloader:\n",
    "    inputs, labels = batch\n",
    "    inputs = inputs.to(device)\n",
    "    labels = labels.to(device)\n",
    "    # ... rest of training loop\n",
    "print(f\"Model device: {next(model.parameters()).device}\")\n",
    "print(f\"Available GPU: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "749c8e41-9c9b-4b05-923b-683516f9bcea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing data collator with padding to max_length=512...\n",
      "‚úÖ Data collator working correctly!\n",
      "Batch keys: dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'labels'])\n",
      "Input IDs shape: torch.Size([2, 512])\n",
      "Attention mask shape: torch.Size([2, 512])\n",
      "Labels shape: torch.Size([2])\n",
      "\n",
      "Sequence lengths in dataset: [10, 10]\n",
      "Min length: 10\n",
      "Max length: 10\n",
      "Average length: 10.0\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "import torch\n",
    "\n",
    "# 1. Initialize tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# 2. Set pad token if missing\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    print(f\"Set pad token to EOS token: {tokenizer.pad_token}\")\n",
    "\n",
    "# 3. Tokenize your dataset with truncation and max_length=512\n",
    "train_dataset = [\n",
    "    tokenizer(\"Hello, this is a test sentence.\", truncation=True, max_length=512),\n",
    "    tokenizer(\"Another example sentence that might be longer.\", truncation=True, max_length=512)\n",
    "]\n",
    "\n",
    "# Add dummy labels (replace with your actual labels)\n",
    "for i, sample in enumerate(train_dataset):\n",
    "    sample[\"label\"] = i % 2\n",
    "\n",
    "# 4. Create data collator that pads to max_length=512 exactly\n",
    "data_collator = DataCollatorWithPadding(\n",
    "    tokenizer=tokenizer,\n",
    "    padding=\"max_length\",   # pad sequences to max_length\n",
    "    max_length=512,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "# 5. Test data collator\n",
    "print(\"Testing data collator with padding to max_length=512...\")\n",
    "try:\n",
    "    batch = data_collator(train_dataset)\n",
    "    print(\"‚úÖ Data collator working correctly!\")\n",
    "    print(f\"Batch keys: {batch.keys()}\")\n",
    "    print(f\"Input IDs shape: {batch['input_ids'].shape}\")          # Should be (2, 512)\n",
    "    print(f\"Attention mask shape: {batch['attention_mask'].shape}\")# Should be (2, 512)\n",
    "    print(f\"Labels shape: {batch['labels'].shape}\")                # Should be (2,)\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Data collator error: {e}\")\n",
    "\n",
    "# 6. Check sequence lengths (should all be 512 now)\n",
    "lengths = [len(sample['input_ids']) for sample in train_dataset]\n",
    "print(f\"\\nSequence lengths in dataset: {lengths}\")\n",
    "print(f\"Min length: {min(lengths)}\")\n",
    "print(f\"Max length: {max(lengths)}\")\n",
    "print(f\"Average length: {sum(lengths)/len(lengths):.1f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cb88f7be-fa93-4b76-bc58-3720fdc5ad9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accelerator state reset successfully\n"
     ]
    }
   ],
   "source": [
    "# Add this FIRST, before any other imports\n",
    "import os\n",
    "import gc\n",
    "import torch\n",
    "\n",
    "# Clear environment variables that might cause issues\n",
    "os.environ.pop(\"ACCELERATE_USE_CPU\", None)\n",
    "os.environ.pop(\"CUDA_VISIBLE_DEVICES\", None)\n",
    "\n",
    "# Clear GPU memory\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.synchronize()\n",
    "gc.collect()\n",
    "\n",
    "# Reset accelerator state\n",
    "try:\n",
    "    from accelerate.state import AcceleratorState\n",
    "    AcceleratorState._reset_state()\n",
    "    print(\"Accelerator state reset successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not reset accelerator state: {e}\")\n",
    "\n",
    "# Now import your other libraries\n",
    "from transformers import Trainer\n",
    "# ... rest of your imports\n",
    "from transformers import Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    processing_class=tokenizer,  # Changed from tokenizer to processing_class\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c7736287-56bd-4955-963b-75b10705f0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_latest_checkpoint(output_dir):\n",
    "    \"\"\"Find the latest checkpoint in the output directory\"\"\"\n",
    "    checkpoint_pattern = os.path.join(output_dir, \"checkpoint-*\")\n",
    "    checkpoints = glob.glob(checkpoint_pattern)\n",
    "    if checkpoints:\n",
    "        # Sort by checkpoint number\n",
    "        checkpoints.sort(key=lambda x: int(x.split('-')[-1]))\n",
    "        latest_checkpoint = checkpoints[-1]\n",
    "        print(f\"Found latest checkpoint: {latest_checkpoint}\")\n",
    "        return latest_checkpoint\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701d3ea1-c58f-4481-9177-65e275a5b78b",
   "metadata": {},
   "source": [
    "üèãÔ∏è Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "faf7a6ba-b977-46e5-938f-900e82b2d254",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üåç Multilingual Offensive Speech Detection Training\n",
      "‚úÖ All datasets loaded successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64a1535907c54d83b294ebb33cb4e199",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/33598 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b31c786ce9ec44159575f8fa58cf3f4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7780 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Dataset debugging:\n",
      "  label: <class 'torch.Tensor'> - 0\n",
      "  input_ids: <class 'torch.Tensor'> - length 15\n",
      "  attention_mask: <class 'torch.Tensor'> - length 15\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10500' max='10500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10500/10500 42:44:35, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.415300</td>\n",
       "      <td>0.759530</td>\n",
       "      <td>0.779820</td>\n",
       "      <td>0.792382</td>\n",
       "      <td>0.779820</td>\n",
       "      <td>0.782524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.035200</td>\n",
       "      <td>0.656911</td>\n",
       "      <td>0.805656</td>\n",
       "      <td>0.814518</td>\n",
       "      <td>0.805656</td>\n",
       "      <td>0.808257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.974900</td>\n",
       "      <td>0.650470</td>\n",
       "      <td>0.771851</td>\n",
       "      <td>0.827395</td>\n",
       "      <td>0.771851</td>\n",
       "      <td>0.783951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.036600</td>\n",
       "      <td>0.651813</td>\n",
       "      <td>0.801799</td>\n",
       "      <td>0.830168</td>\n",
       "      <td>0.801799</td>\n",
       "      <td>0.809189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.823700</td>\n",
       "      <td>0.697965</td>\n",
       "      <td>0.811054</td>\n",
       "      <td>0.830382</td>\n",
       "      <td>0.811054</td>\n",
       "      <td>0.816513</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Training completed and model saved!\n",
      "\n",
      "üìä Final Evaluation:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='487' max='487' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [487/487 18:22]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  eval_loss: 0.6980\n",
      "  eval_accuracy: 0.8111\n",
      "  eval_precision: 0.8304\n",
      "  eval_recall: 0.8111\n",
      "  eval_f1: 0.8165\n",
      "  eval_runtime: 1104.6624\n",
      "  eval_samples_per_second: 7.0430\n",
      "  eval_steps_per_second: 0.4410\n",
      "  epoch: 5.0000\n",
      "üéâ Done!\n"
     ]
    }
   ],
   "source": [
    "# ‚úÖ Required imports\n",
    "import os\n",
    "import glob\n",
    "import torch\n",
    "import gc\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from accelerate import Accelerator\n",
    "from transformers import (\n",
    "    Trainer, \n",
    "    TrainingArguments,\n",
    "    AutoTokenizer, \n",
    "    AutoModelForSequenceClassification,\n",
    "    DataCollatorWithPadding,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from datasets import Dataset\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ‚úÖ Checkpoint finder\n",
    "def find_latest_checkpoint(output_dir):\n",
    "    checkpoint_pattern = os.path.join(output_dir, \"checkpoint-*\")\n",
    "    checkpoints = glob.glob(checkpoint_pattern)\n",
    "    if checkpoints:\n",
    "        checkpoints.sort(key=lambda x: int(x.split('-')[-1]))\n",
    "        return checkpoints[-1]\n",
    "    return None\n",
    "\n",
    "# ‚úÖ Clear memory\n",
    "def clear_memory():\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()\n",
    "    gc.collect()\n",
    "\n",
    "\n",
    "# ‚úÖ Trainer creator\n",
    "# ‚úÖ Custom trainer with class weights\n",
    "class WeightedTrainer(Trainer):\n",
    "    def __init__(self, class_weights=None, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.class_weights = class_weights\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get('logits')\n",
    "\n",
    "        if self.class_weights is not None:\n",
    "            weight_tensor = torch.tensor(list(self.class_weights.values()), \n",
    "                                         dtype=torch.float32, device=logits.device)\n",
    "            loss_fct = torch.nn.CrossEntropyLoss(weight=weight_tensor)\n",
    "        else:\n",
    "            loss_fct = torch.nn.CrossEntropyLoss()\n",
    "        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "# ‚úÖ Trainer creator (fix parameter name tokenizer)\n",
    "def create_trainer(model, training_args, train_dataset, val_dataset, tokenizer, data_collator, compute_metrics, class_weights=None):\n",
    "    return WeightedTrainer(\n",
    "        class_weights=class_weights,\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        tokenizer=tokenizer,                 # Fixed from processing_class=tokenizer\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    "    )\n",
    "\n",
    "# ‚úÖ Metrics\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    from sklearn.metrics import precision_recall_fscore_support\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='weighted', zero_division=0)\n",
    "    return {\n",
    "        \"accuracy\": (predictions == labels).mean(),\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1\n",
    "    }\n",
    "\n",
    "# ‚úÖ Label encoder\n",
    "def encode_labels(df, label_mapping=None):\n",
    "    df = df.copy()\n",
    "    df['label'] = df['label'].astype(str).str.strip().str.lower()\n",
    "    unique_labels = list(df['label'].unique())\n",
    "    unique_labels.sort()\n",
    "    if label_mapping is None:\n",
    "        label_mapping = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "    df['label'] = df['label'].map(label_mapping)\n",
    "    df = df.dropna(subset=['label'])\n",
    "    df['label'] = df['label'].astype(int)\n",
    "    return df, label_mapping\n",
    "\n",
    "# ‚úÖ Data loader\n",
    "def load_and_preprocess_data():\n",
    "    try:\n",
    "        tamil_train = pd.read_csv(\"tamil_offensive_speech_train.csv\")[[\"comment\", \"label\"]].rename(columns={'comment': 'text'})\n",
    "        tamil_val = pd.read_csv(\"tamil_offensive_speech_val.csv\")[[\"comment\", \"label\"]].rename(columns={'comment': 'text'})\n",
    "        tamil_train[\"lang\"] = \"ta\"\n",
    "        tamil_val[\"lang\"] = \"ta\"\n",
    "\n",
    "        hindi_train = pd.read_csv(\"Hatespeech-Hindi_Train.csv\")[[\"Post\", \"Labels Set\"]].rename(columns={'Post': 'text', 'Labels Set': 'label'})\n",
    "        hindi_val = pd.read_csv(\"Hatespeech-Hindi_Valid.csv\")[[\"Post\", \"Labels Set\"]].rename(columns={'Post': 'text', 'Labels Set': 'label'})\n",
    "        hindi_train[\"lang\"] = \"hi\"\n",
    "        hindi_val[\"lang\"] = \"hi\"\n",
    "\n",
    "        print(\"‚úÖ All datasets loaded successfully\")\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"‚ùå Error loading datasets: {e}\")\n",
    "        return None, None, None\n",
    "\n",
    "    train_df = pd.concat([tamil_train, hindi_train], ignore_index=True).dropna().reset_index(drop=True)\n",
    "    val_df = pd.concat([tamil_val, hindi_val], ignore_index=True).dropna().reset_index(drop=True)\n",
    "    train_df, label_mapping = encode_labels(train_df)\n",
    "    val_df, _ = encode_labels(val_df, label_mapping)\n",
    "\n",
    "    class_weights = compute_class_weight('balanced', classes=np.unique(train_df['label']), y=train_df['label'])\n",
    "    class_weight_dict = {i: weight for i, weight in enumerate(class_weights)}\n",
    "    return train_df, val_df, len(set(train_df['label'].unique())), class_weight_dict\n",
    "\n",
    "# ‚úÖ Main training logic\n",
    "def run_training(model, training_args, train_dataset, val_dataset, tokenizer, data_collator, compute_metrics, class_weights=None):\n",
    "    print(f\"üîç Dataset debugging:\")\n",
    "    sample_item = train_dataset[0]\n",
    "    for key, value in sample_item.items():\n",
    "        if key == 'label':\n",
    "            display_val = value\n",
    "        else:\n",
    "            display_val = f\"length {len(value)}\" if hasattr(value, '__len__') else \"length N/A\"\n",
    "        print(f\"  {key}: {type(value)} - {display_val}\")\n",
    "\n",
    "    latest_checkpoint = find_latest_checkpoint(\"./results\")\n",
    "    if latest_checkpoint and not os.path.exists(os.path.join(latest_checkpoint, \"trainer_state.json\")):\n",
    "        latest_checkpoint = None\n",
    "\n",
    "    trainer = create_trainer(model, training_args, train_dataset, val_dataset, tokenizer, data_collator, compute_metrics, class_weights)\n",
    "    retry_count, max_retries = 0, 3\n",
    "    while retry_count < max_retries:\n",
    "        try:\n",
    "            if latest_checkpoint and retry_count == 0:\n",
    "                trainer.train(resume_from_checkpoint=latest_checkpoint)\n",
    "            else:\n",
    "                trainer.train()\n",
    "            trainer.save_model(\"./final_model\")\n",
    "            tokenizer.save_pretrained(\"./final_model\")\n",
    "            print(\"‚úÖ Training completed and model saved!\")\n",
    "            return trainer\n",
    "        except RuntimeError as e:\n",
    "            if \"out of memory\" in str(e).lower():\n",
    "                retry_count += 1\n",
    "                training_args.per_device_train_batch_size = max(1, training_args.per_device_train_batch_size // 2)\n",
    "                training_args.per_device_eval_batch_size = max(1, training_args.per_device_eval_batch_size // 2)\n",
    "                del trainer\n",
    "                clear_memory()\n",
    "                time.sleep(5)\n",
    "                trainer = create_trainer(model, training_args, train_dataset, val_dataset, tokenizer, data_collator, compute_metrics, class_weights)\n",
    "                print(f\"Retrying with reduced batch sizes. Attempt: {retry_count}\")\n",
    "            else:\n",
    "                raise\n",
    "        except KeyboardInterrupt:\n",
    "            print(\"‚èπÔ∏è Training interrupted. Progress saved.\")\n",
    "            break\n",
    "    return None\n",
    "\n",
    "# ‚úÖ Main\n",
    "def main():\n",
    "    print(\"üåç Multilingual Offensive Speech Detection Training\")\n",
    "    train_df, val_df, num_labels, class_weights = load_and_preprocess_data()\n",
    "    if train_df is None:\n",
    "        return\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-base\")\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\"xlm-roberta-base\", num_labels=num_labels)\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./checkpoints\",\n",
    "        save_strategy=\"epoch\",          # save after each epoch\n",
    "        eval_strategy=\"epoch\",    # evaluate after each epoch\n",
    "        save_total_limit=3,\n",
    "        load_best_model_at_end=True,\n",
    "        greater_is_better=True,\n",
    "        num_train_epochs=5,\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=16,\n",
    "        warmup_steps=500,\n",
    "        weight_decay=0.01,\n",
    "        learning_rate=2e-5,\n",
    "        logging_dir='./logs',\n",
    "        logging_steps=100,\n",
    "        metric_for_best_model=\"f1\",\n",
    "        report_to=[],\n",
    "        dataloader_pin_memory=False,\n",
    "        fp16=torch.cuda.is_available(),\n",
    "        dataloader_num_workers=4 if torch.cuda.is_available() else 0,\n",
    "        remove_unused_columns=True,\n",
    ")\n",
    "\n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "    def tokenize_function(examples):\n",
    "        return tokenizer(examples['text'], truncation=True, padding=False, max_length=256)\n",
    "\n",
    "    train_dataset = Dataset.from_pandas(train_df).map(tokenize_function, batched=True, remove_columns=[\"text\", \"lang\"])\n",
    "    val_dataset = Dataset.from_pandas(val_df).map(tokenize_function, batched=True, remove_columns=[\"text\", \"lang\"])\n",
    "    train_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "    val_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "\n",
    "    trainer = run_training(model, training_args, train_dataset, val_dataset, tokenizer, data_collator, compute_metrics, class_weights)\n",
    "\n",
    "    if trainer:\n",
    "        print(\"\\nüìä Final Evaluation:\")\n",
    "        results = trainer.evaluate()\n",
    "        for key, value in results.items():\n",
    "            print(f\"  {key}: {value:.4f}\")\n",
    "\n",
    "    print(\"üéâ Done!\")\n",
    "\n",
    "# ‚úÖ Run\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0da5932-fc8a-4069-90fb-bec5b9e64e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_prediction(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "        predicted_class = torch.argmax(predictions, dim=-1).item()\n",
    "    return predicted_class, predictions[0].tolist()\n",
    "# Example usage - uncomment to test\n",
    "# test_text = \"This is a sample text\"\n",
    "# pred_class, confidence = test_prediction(test_text)\n",
    "# print(f\"Predicted class: {pred_class}, Confidence: {confidence}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683906fa-0cea-4378-b574-e68a47236047",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "predictions = trainer.predict(val_dataset)\n",
    "pred_labels = predictions.predictions.argmax(axis=1)\n",
    "\n",
    "print(classification_report(val_labels, pred_labels, target_names=label_map.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f66796-d24f-4b04-a257-4f4721a8efaf",
   "metadata": {},
   "source": [
    "‚úÖ Conclusion\n",
    "- Multilingual model trained using Hindi and Tamil data\n",
    "- Powered by XLM-RoBERTa for cross-lingual learning\n",
    "- Ready to deploy as REST API or chatbot"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
